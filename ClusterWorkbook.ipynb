{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-Means Clustering\n",
    "Purpose: Find clusters and their outliers, output the results to np arrays.\n",
    "\n",
    "Required:\n",
    ".pkl file produced from keplerml.py.\n",
    "\n",
    "This was created with the intent to cluster lightcurves based on the features calculated by keplerml.py. However,\n",
    "the methodology used is general and can be applied to any set of data as long as the data is saved as a pickle file\n",
    "with the identifier (file name) as the [:,0] index, and the rest of the data for each file as the [:,1:] \n",
    "indices.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pyfits\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import scipy.signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from numpy.random import RandomState\n",
    "from multiprocessing import Pool\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.widgets import RadioButtons\n",
    "import os\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "if sys.version_info[0] < 3:\n",
    "    from Tkinter import Tk\n",
    "else:\n",
    "    from tkinter import Tk\n",
    "\n",
    "from tkFileDialog import askopenfilename,askdirectory\n",
    "\n",
    "# Transposition. Will change an array orgnaized by data point (coordinates) to an array organized \n",
    "# by feature (list of all values for each feature) and vice versa.\n",
    "def reorganizeArray(X):\n",
    "    return [[X[i][j] for i in range(len(X))] for j in range(len(X[0]))]\n",
    "\n",
    "def bounding_box(X):\n",
    "    # X is the data that comes in, it's organized by lightcurve[[all features for lc 1],[features for lc2],...]\n",
    "    # To find minima we need to consider all points for each feature seperate from the other features.\n",
    "    Xbyfeature = reorganizeArray(X)\n",
    "\n",
    "    # xmin/xmax will be an array of the minimum/maximum values of the features\n",
    "    xmin=[]\n",
    "    xmax=[]\n",
    "    # Create the minimum vertex, and the maximum vertex for the box\n",
    "    for feature in range(60):\n",
    "        xmin.append(min(Xbyfeature[feature]))\n",
    "        xmax.append(max(Xbyfeature[feature]))\n",
    "        \n",
    "    return (xmin,xmax)\n",
    "\n",
    "def KMeans_clusters(data,nclusters):\n",
    "    # Run KMeans, get clusters\n",
    "    npdata = np.array(data)\n",
    "    est = KMeans(n_clusters=nclusters)\n",
    "    est.fit(npdata)\n",
    "    clusters = est.labels_\n",
    "    centers = est.cluster_centers_\n",
    "    return clusters, centers\n",
    "\n",
    "def seperatedClusters(data,nclusters,clusters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    data - all data, organized by lightcurve \n",
    "    nclusters - number of clusters\n",
    "    clusters - cluster labels\n",
    "    \n",
    "    Purpose: Create arrays dataByCluster and clusterIndexes containing data seperated by\n",
    "    by cluster.\n",
    "    \"\"\"\n",
    "    dataByCluster = []\n",
    "    clusterIndexes = []\n",
    "    \n",
    "    # Will try to stick to the following:\n",
    "    # cluster i, lightcurve j, feature k\n",
    "    for i in range(nclusters):\n",
    "        # Keeping track of which points get pulled into each cluster:\n",
    "        clusterIndexes.append([j for j in range(len(data)) if clusters[j]==i])\n",
    "        # Separating the clusters out into their own arrays (w/in the cluster array)\n",
    "        dataByCluster.append([data[clusterIndexes[i][j]] for j in range(len(clusterIndexes[i]))])\n",
    "        \n",
    "    return dataByCluster, clusterIndexes\n",
    "\n",
    "def distances(pointsForDistance,referencePoint):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    cluster/pointsForDistance - array of any number of points, each point an array of it's features\n",
    "    centerloc/referencePoint - array of the features for a single point\n",
    "    \n",
    "    Purpose: \n",
    "    This will calculate the distances of a group of points to a given point.\n",
    "    I should change the reference point to the center of the cluster rather than the point closest to the center.\n",
    "    \n",
    "    \"\"\"\n",
    "    distFromCenter = [0 for j in range(len(pointsForDistance))] # reinitializing for each cluster\n",
    "    # loop for each lightcurve of the cluster\n",
    "    for j in range(len(pointsForDistance)):\n",
    "        dataloc=pointsForDistance[j] # coordinates of the datapoint\n",
    "        sqrd=0\n",
    "        # loop for each feature of the lightcurve \n",
    "        for k in range(len(referencePoint)):\n",
    "            sqrd+=pow(dataloc[k]-referencePoint[k],2) # (x-x0)^2+(y-y0)^2+...\n",
    "        distance = pow(sqrd,0.5) # sqrt((x-x0)^2+(y-y0)^2+...)\n",
    "        distFromCenter[j]=distance \n",
    "    return distFromCenter\n",
    "\n",
    "def beyondCutoff(cutoff,distances):\n",
    "    \"\"\"\n",
    "    returns: outliers and typical arrays\n",
    "        outlier array contains indexes of the specific cluster array that are beyond the cutoff\n",
    "        typical array contains single index of the cluster array that is nearest the center\n",
    "    \n",
    "    Args:\n",
    "    cutoff - a number, everything beyond this number is considered an outlier\n",
    "    distances - array containing distances to each lightcurve point from the center for a given cluster\n",
    "    \"\"\"\n",
    "    outliers=[j for j in range(len(distances)) if distances[j]>=cutoff] # recalculated for each cluster\n",
    "    typical=[j for j in range(len(distances)) if distances[j]==min(distances)]\n",
    "    return outliers,typical\n",
    "\n",
    "def outliers(data,clusters,centers,files):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    data - all the data\n",
    "    clusters - the cluster labels from kmeans. DBSCAN will require different methodology\n",
    "    centers - locations of the cluster centers\n",
    "    \n",
    "    Purpose:\n",
    "    Separate out the data on the edge of the clusters which are the most likely anomalous data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nclusters = len(centers)\n",
    "\n",
    "    # Sanity check, if this isn't an outlier than something is wrong.\n",
    "    \"\"\"\n",
    "    controlPoint = np.array([10000 for i in range(len(data[0]))])\n",
    "    \n",
    "    data=np.append(data,[controlPoint],axis=0)\n",
    "    clusters=np.append(clusters,[1],axis=0)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster, clusterIndexes = seperatedClusters(data,nclusters,clusters)\n",
    "    clusterIndexes = np.array(clusterIndexes)\n",
    "    twoSigma = [] #probably doesn't need an array \n",
    "    distFromCenter = []\n",
    "    allTypical=[]\n",
    "    allOutliers=[]\n",
    "    # Will try to stick to the following:\n",
    "    # cluster i, lightcurve j, feature k\n",
    "    for i in range(nclusters):\n",
    "                \n",
    "        \"\"\"\n",
    "        ========== Checking density of clusters ============\n",
    "        Given that some of the clusters may have 0 spread in a given feature, I'm not confident that this is meaningful,\n",
    "        but there's a chance this can give insight into potentially outlying clusters (clusters of misfits)\n",
    "        \"\"\"\n",
    "        (xmin,xmax)=bounding_box(cluster[i]) # get the minimum and maximum values for both\n",
    "        \n",
    "        diff=[xmax[n]-xmin[n] for n in range(len(xmin))]\n",
    "        volOfClusterBB=1.0\n",
    "        for n in diff:\n",
    "            if n!=0:\n",
    "                volOfClusterBB*=n\n",
    "        densOfCluster=len(cluster[i])/volOfClusterBB\n",
    "        \"\"\"\n",
    "        print(\"Cluster: %s, Density: %s\" %(i,densOfCluster))\n",
    "        print volOfClusterBB\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ========== Finding points outside of the cutoff ===========\n",
    "        \"\"\"\n",
    "        # not currently using this cutoff.\n",
    "        twoSigma.append(2*np.std(cluster[i]))\n",
    "        \n",
    "        \"\"\"\n",
    "            ==== Calculating distances to each point ====\n",
    "        \"\"\"\n",
    "        # Calculate distances to each point in each cluster to the center of its cluster\n",
    "        distFromCenter.append(distances(cluster[i],centers[i]))\n",
    "        \n",
    "        \"\"\"\n",
    "            ==== Finding outliers and the standard (defined by the closest to the center) ====\n",
    "        \"\"\"\n",
    "        cutoff=.5*max(distFromCenter[i])\n",
    "        # Will implement a better cutoff.\n",
    "        \n",
    "        # returns cluster indices of the outliers and the typical lcs\n",
    "        outliers,typical = beyondCutoff(cutoff,distFromCenter[i])\n",
    "        \n",
    "        # Add typical lightcurve to list. Only 1 produced at present, but this may change. The following\n",
    "        # accounts for adding more typicals later.\n",
    "        clusterIndexes[i] = np.array(clusterIndexes[i])\n",
    "        allTypical.append(clusterIndexes[i][typical])\n",
    "            \n",
    "        # place outliers from this cluster into general outlier list\n",
    "        allOutliers.append(clusterIndexes[i][outliers])\n",
    "        \n",
    "    return allOutliers,allTypical\n",
    "\n",
    "\"\"\"\n",
    "============ Start ============\n",
    "\"\"\"\n",
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "output = askopenfilename() # locate the pickle file with the relevant data\n",
    "\n",
    "if output:\n",
    "    \n",
    "    pathtofile = os.path.dirname(output)\n",
    "    outputfile = open(output,'r+') # show an \"Open\" dialog box and return the path to the selected file\n",
    "    outputdata = []\n",
    "    while True:\n",
    "        try:\n",
    "            o = pickle.load(outputfile)\n",
    "        except EOFError:\n",
    "            break\n",
    "        else:\n",
    "            outputdata.append(o)\n",
    "    outputfile.close()\n",
    "    \n",
    "\n",
    "    files = [outputdata[i][0] for i in range(len(outputdata)) if not np.isnan(outputdata[i][1:]).any()]\n",
    "    data = np.array([outputdata[i][1:] for i in range(len(outputdata)) if not np.isnan(outputdata[i][1:]).any()])\n",
    "    \n",
    "    # files with NaN will automatically be outliers\n",
    "    filesNaN = [outputdata[i][0] for i in range(len(outputdata)) if np.isnan(outputdata[i][1:]).any()]\n",
    "    dataNaN = np.array([outputdata[i][1:] for i in range(len(outputdata)) if np.isnan(outputdata[i][1:]).any()])\n",
    "    \n",
    "    # nclusters can be obtained through the optimalK.py script \n",
    "    nclusters = int(raw_input('Enter the number of clusters expected: '))\n",
    "\n",
    "    clusterLabels,centers=KMeans_clusters(data,nclusters)\n",
    "    dataByCluster,clusterIndexes = seperatedClusters(data,nclusters,clusterLabels)\n",
    "    outliers,typical=outliers(data,clusterLabels,centers,files)\n",
    "    \n",
    "    \"\"\"\n",
    "    Saving all cluster output in a single pickle file.\n",
    "    format: [filename array,cluster label array,data array array]\n",
    "    \"\"\"\n",
    "    clusterLabels = clusterLabels+1\n",
    "    for i in typical:\n",
    "        clusterLabels[i] = 0\n",
    "        nclusters = max(clusterLabels)\n",
    "    for i in outliers:\n",
    "        clusterLabels[i] = nclusters+1\n",
    "    for i in range(len(filesNaN)):\n",
    "        files.append(filesNaN[i])\n",
    "        np.append(data,dataNaN[i])\n",
    "        np.append(clusterLabels,nclusters+1)\n",
    "\n",
    "    clusteroutputarray = [files,clusterLabels,data]\n",
    "    clusteroutput = pathtofile+'/'+raw_input('Save as: ')+'.pkl'\n",
    "    # Tests to see if output has been created previously. Allows the user to replace the existing cluster data or abort.\n",
    "    if os.path.isfile(clusteroutput):\n",
    "        while os.path.isfile(clusteroutput):\n",
    "            replace = raw_input('Cluster output file exists, replace? (y/n) ')\n",
    "            while(replace!=('y' or 'n')):\n",
    "                if replace == 'y':\n",
    "                    clusterfile = open(clusteroutput,'r+')\n",
    "                elif replace == 'n':\n",
    "                    clusteroutput = pathtofile+'/'+raw_input('Enter new file name: ')+'.pkl'\n",
    "                replace = raw_input(\"Invalid response (y/n): \")\n",
    "    else:\n",
    "        open(clusteroutput,'a').close()\n",
    "    clusterfile = open(clusteroutput,'r+')\n",
    "    pickle.dump(clusteroutputarray,clusterfile)\n",
    "    clusterfile.close()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which clustering method (kmeans or dbscan)? kmeans\n",
      "Enter the identifier (including cluster #) of the data (ex. Q1_c1): Q5_c1\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/home/dgiles/Documents/KeplerLCs/output/Q5_ClusterOutput/Q5_c1dataByLightCurve.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3b839b880009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter the identifier (including cluster #) of the data (ex. Q1_c1): '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mdataID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathtofile\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'dataByLightCurve.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mffeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreorganizeArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathtofile\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'filelist'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dgiles/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/dgiles/Documents/KeplerLCs/output/Q5_ClusterOutput/Q5_c1dataByLightCurve.npy'"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "This will plot individual clusters into a 3D scatter and on choosing (clicking on) a point, \n",
    "plot the point's associated lightcurve. The 3 axes will be selectable via radiobuttons.\n",
    "\n",
    "This may be applied to any scatter of data with associated lightcurves (assuming the \n",
    "lightcurve data is available and the filelist appropriately named).\n",
    "\n",
    "TODO: Make this general, plot DBSCAN or KMeans generated clusters.\n",
    "Want to plot uniquely for Kmeans and DBSCAN\n",
    "\n",
    "Requirements:\n",
    "Filelist - Containing file names of the lightcurve data for the cluster\n",
    "Data file - numpy array containing calculated features for each lightcurve\n",
    "Optional:\n",
    "Outlier filelist - contains the file names for the outliers and the typical* lightcurve. \n",
    "    The typical lightcurve must be on line number 1.\n",
    "\n",
    "*typical as defined by closest to the center of the cluster\n",
    "\n",
    "\n",
    "Example file list:\n",
    "    Name: exfilelist\n",
    "    Content:\n",
    "    lightcurve1_llc.fits\n",
    "    lightcurve2_llc.fits\n",
    "    lightcurve3_llc.fits\n",
    "\n",
    "Note: There can be no blank lines in the filelist.\n",
    "\n",
    "Example numpy file:\n",
    "    Name: exdataByLightCurve.npy\n",
    "    Content (when read-in by np.load()):\n",
    "    [[data for lightcurve1]\n",
    "     [data for lightcurve2]\n",
    "     [data for lightcurve3]]\n",
    " \n",
    "Note: The filelist and the .npy must have the above fromat and must be organized in the same way,\n",
    "e.g. the first index of the array corresponds to the first line of the filelist.\n",
    "\n",
    "filelist: identifier+'filelist'    npy: identifier+'dataByLighCurve.npy'\n",
    "In the example above the identifier was 'ex'.\n",
    "\"\"\"\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pyfits\n",
    "import math\n",
    "# The order of the following is important, matplot.use('TkAgg') must be done before importing pyplot\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D,proj3d\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import Tkinter as Tk\n",
    "else:\n",
    "    import tkinter as Tk\n",
    "    \n",
    "from tkFileDialog import askopenfilename,askdirectory\n",
    "\n",
    "root = Tk.Tk()\n",
    "root.wm_title(\"Scatter\")\n",
    "\n",
    "def reorganizeArray(X): \n",
    "    # This is just a transposition, could be accomplished by making X an np.array then transposing via X.T if prefered\n",
    "    # TODO: Time the difference with large datasets (100,000+)\n",
    "    return [[X[i][j] for i in range(len(X))] for j in range(len(X[0]))]\n",
    "\n",
    "def DBSCAN_clusters(data,eps,min_points=None):\n",
    "    npdata = np.array(data)\n",
    "    if min_points==None:\n",
    "        est = DBSCAN(eps=eps)\n",
    "    else:\n",
    "        est = DBSCAN(eps=eps,min_samples=min_points)\n",
    "    \n",
    "    est.fit(npdata)\n",
    "    clusters = est.labels_\n",
    "    coreSampleIndices = est.core_sample_indices_\n",
    "    return clusters, coreSampleIndices\n",
    "\n",
    "\"\"\"--- Importing the relevant data ---\"\"\"\n",
    "clusterType = raw_input('Which clustering method (kmeans or dbscan)? ')\n",
    "if clusterType == 'KMeans' or clusterType == 'kmeans' or clusterType == 'Kmeans': \n",
    "    #Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "    print(\"Choose the directory the cluster data is contained in.\")\n",
    "    pathtofile = askdirectory()+'/' # Choose the directory containing the cluster files\n",
    "    \n",
    "    identifier = raw_input('Enter the identifier (including cluster #) of the data (ex. Q1_c1): ')\n",
    "    dataID = pathtofile+str(identifier)+'dataByLightCurve.npy'\n",
    "    data = np.load(dataID)\n",
    "    ffeatures=reorganizeArray(data)\n",
    "    filelist = pathtofile+str(identifier)+'filelist'\n",
    "    outlierfilelist = pathtofile+str(identifier)+'outlierfilelist'\n",
    "    files = [line.strip() for line in open(filelist)]\n",
    "    outlierfiles = [line.strip() for line in open(outlierfilelist)]\n",
    "    centerIndex = files.index(outlierfiles[0])\n",
    "    outlierIndices = []\n",
    "    for f in outlierfiles:\n",
    "        outlierIndices.append(files.index(f))\n",
    "        \n",
    "    # Default color blue\n",
    "    colors = ['b' for k in range(len(files))]\n",
    "\n",
    "    # Outliers set to red\n",
    "    for i in outlierIndices:\n",
    "        colors[i] ='r'\n",
    "\n",
    "    # Center of cluster set to green\n",
    "    colors[centerIndex]='g'\n",
    "    \n",
    "elif clusterType == 'DBSCAN' or clusterType =='dbscan':\n",
    "    identifier = raw_input('Enter the identifier of the data: ')\n",
    "    dataID = 'input/'+str(identifier)+'dataByLightCurve.npy'\n",
    "    data = np.load(dataID)\n",
    "    ffeatures=reorganizeArray(data)\n",
    "    filelist = 'input/'+str(identifier)+'filelist'\n",
    "    files = [line.strip() for line in open(filelist)]\n",
    "    clusterLabels,coreSampleIndices = DBSCAN_clusters(data,80,10)\n",
    "    centerIndex=coreSampleIndices[0]\n",
    "    outlierIndices = [i for i in range(len(clusterLabels))if clusterLabels[i]==-1]\n",
    "    outlierFiles = [files[i] for i in outlierIndices]\n",
    "    numClusters = max(clusterLabels)\n",
    "    colors = ['b' for k in range(len(files))]\n",
    "    for i in outlierIndices:\n",
    "        colors[i] = 'r'\n",
    "    for i in coreSampleIndices:\n",
    "        colors[i] = 'g'\n",
    "\n",
    "else:\n",
    "    print(\"Only KMeans and DBSCAN are compatible with this program.\")\n",
    "    sys.exit()\n",
    "\n",
    "def read_kepler_curve(file):\n",
    "    lc = pyfits.getdata(file)\n",
    "    t = lc.field('TIME')\n",
    "    f = lc.field('PDCSAP_FLUX')\n",
    "    err = lc.field('PDCSAP_FLUX_ERR')\n",
    "    f = f[np.isfinite(t)]\n",
    "    t = t[np.isfinite(t)]\n",
    "    t = t[np.isfinite(f)]\n",
    "    f = f[np.isfinite(f)]\n",
    "    \n",
    "    nf = f / np.median(f)\n",
    "\n",
    "    return t, nf, err\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"--- import light curve data ---\"\"\"\n",
    "    tArr = []\n",
    "    nfArr = []\n",
    "    print(\"Choose the directory that the fits files are contained in.\")\n",
    "    #Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "    pathtofits = askdirectory()+'/' # Choose the directory containing the fits files\n",
    "    for f in files:\n",
    "        f = pathtofits+f\n",
    "        t,nf,err=read_kepler_curve(f) #t, nf, err arrays of each for the specific lc\n",
    "        tArr.append(t)\n",
    "        nfArr.append(nf)\n",
    "    # tArr and nfArr both arrays containing arrays. tArr[0] contains the array for time of the first lc\n",
    "    lightcurveData = np.array([tArr,nfArr]) # Array of the 2 arrays, lightcurveData[0][0] is tArr[0]\n",
    "    \n",
    "    \"\"\"--- Organizing data and Labels ---\"\"\"\n",
    "    \n",
    "    ffeatures = reorganizeArray(data)\n",
    "\n",
    "    # Betterlabels will contain the titles of axes we'll actually want on there\n",
    "    listoffeatures = ['longtermtrend', 'meanmedrat', 'skews', 'varss', 'coeffvar', 'stds',\\\n",
    "                      'numoutliers', 'numnegoutliers', 'numposoutliers', 'numout1s', 'kurt',\\\n",
    "                      'mad', 'maxslope', 'minslope', 'meanpslope', 'meannslope', 'g_asymm',\\\n",
    "                      'rough_g_asymm', 'diff_asymm', 'skewslope', 'varabsslope', 'varslope',\\\n",
    "                      'meanabsslope', 'absmeansecder', 'num_pspikes', 'num_nspikes', 'num_psdspikes',\\\n",
    "                      'num_nsdspikes','stdratio', 'pstrend', 'num_zcross', 'num_pm', 'len_nmax',\\\n",
    "                      'len_nmin','mautocorrcoef', 'ptpslopes', 'periodicity', 'periodicityr',\\\n",
    "                      'naiveperiod', 'maxvars','maxvarsr', 'oeratio', 'amp', 'normamp','mbp',\\\n",
    "                      'mid20', 'mid35', 'mid50', 'mid65', 'mid80', 'percentamp', 'magratio',\\\n",
    "                      'sautocorrcoef', 'autocorrcoef', 'flatmean', 'tflatmean', 'roundmean',\\\n",
    "                      'troundmean', 'roundrat', 'flatrat']\n",
    "    \n",
    "    betterlabels = ['longtermtrend', 'meanmedrat', 'skews', 'varss', 'coeffvar', 'stds', 'numoutliers',\\\n",
    "                    'numnegoutliers', 'numposoutliers', 'numout1s', 'kurt', 'mad', 'maxslope',\\\n",
    "                    'minslope', 'meanpslope', 'meannslope', 'g_asymm', 'rough_g_asymm', 'diff_asymm',\\\n",
    "                    'skewslope', 'varabsslope', 'varslope', 'meanabsslope', 'absmeansecder', 'num_pspikes',\\\n",
    "                    'Number of Negative Spikes (Slope > 3*sigma)', 'num_psdspikes', 'num_nsdspikes',\\\n",
    "                    'stdratio', 'pstrend', 'Number of Longterm Trendline Crossings', 'Number of Peaks',\\\n",
    "                    'len_nmax', 'len_nmin', 'mautocorrcoef', 'ptpslopes', 'periodicity', 'periodicityr',\\\n",
    "                    'naiveperiod', 'maxvars', 'maxvarsr', 'oeratio', 'amp', 'normamp','mbp', 'mid20',\\\n",
    "                    'mid35', 'mid50', 'mid65', 'mid80', 'percentamp', 'magratio', 'sautocorrcoef',\\\n",
    "                    'autocorrcoef', 'flatmean', 'tflatmean', 'roundmean', 'troundmean', 'roundrat', 'flatrat']\n",
    "    \n",
    "    # labellist keeps track of what each axis should be labelled\n",
    "    # initialized with some interesting axes (based on past experience)\n",
    "    labellist=[betterlabels[30],betterlabels[31],betterlabels[25]]\n",
    "    \n",
    "    # axesdict could probably be accomoplished with a list, see the new list currentData,\n",
    "    # I'm partial to the dictionary because it helps me keep things associated directly.\n",
    "    \n",
    "    axesdict = {'xaxis':ffeatures[30],'yaxis':ffeatures[31],'zaxis':ffeatures[25]}    \n",
    "    fig = Figure()\n",
    "    \n",
    "    # a tk.DrawingArea\n",
    "    canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "    canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "    \n",
    "    # empty subplot for scattered data\n",
    "    ax = fig.add_subplot(211, projection='3d')\n",
    "    plt.subplots_adjust(left=0.3)\n",
    "    ax.mouse_init()\n",
    "    \n",
    "    # empty subplot for lightcurves\n",
    "    ax2 = fig.add_subplot(212)\n",
    "\n",
    "    \n",
    "    # tk text input boxes\n",
    "    e1 = Tk.Entry(master=root)\n",
    "    e1.insert(0,'30')\n",
    "    e1.pack(side=Tk.LEFT)\n",
    "    e2 = Tk.Entry(master=root)\n",
    "    e2.insert(0,'31')\n",
    "    e2.pack(side=Tk.LEFT)\n",
    "    e3 = Tk.Entry(master=root)\n",
    "    e3.insert(0,'25')\n",
    "    e3.pack(side=Tk.LEFT)\n",
    "    \n",
    "    def distance(point, event):\n",
    "        \"\"\"Return distance between mouse position and given data point\n",
    "\n",
    "        Args:\n",
    "            point (np.array): np.array of shape (3,), with x,y,z in data coords\n",
    "            event (MouseEvent): mouse event (which contains mouse position in .x and .xdata)\n",
    "        Returns:\n",
    "            distance (np.float64): distance (in screen coords) between mouse pos and data point\n",
    "        \"\"\"\n",
    "        assert point.shape == (3,), \"distance: point.shape is wrong: %s, must be (3,)\" % point.shape\n",
    "\n",
    "        # Project 3d data space to 2d data space\n",
    "        x2, y2, _ = proj3d.proj_transform(point[0], point[1], point[2], ax.get_proj())\n",
    "        # Convert 2d data space to 2d screen space\n",
    "        x3, y3 = ax.transData.transform((x2, y2))\n",
    "\n",
    "        return np.sqrt ((x3 - event.x)**2 + (y3 - event.y)**2)\n",
    "    \n",
    "    def calcClosestDatapoint(XT, event):\n",
    "        \"\"\"\"Calculate which data point is closest to the mouse position.\n",
    "        \n",
    "        Args:\n",
    "            XT (np.array) - array of points, of shape (numPoints, 3)\n",
    "            event (MouseEvent) - mouse event (containing mouse position)\n",
    "        Returns:\n",
    "            smallestIndex (int) - the index (into the array of points X) of the element closest to the mouse position\n",
    "        \"\"\"\n",
    "        distances = [distance (XT[i,:], event) for i in range(XT.shape[0])]\n",
    "        return np.argmin(distances)\n",
    "    \n",
    "    def drawData(X, index):\n",
    "        # Plots the lightcurve of the point chosen\n",
    "        ax2.cla()\n",
    "        x = X[0][index] \n",
    "        y = X[1][index]\n",
    "        axrange=0.55*(max(y)-min(y))\n",
    "        mid=(max(y)+min(y))/2\n",
    "        yaxmin = mid-axrange\n",
    "        yaxmax = mid+axrange\n",
    "        ax2.set_ylim(yaxmin,yaxmax)\n",
    "        \n",
    "        p1,=ax2.plot(X[0][centerIndex], X[1][centerIndex], 'o',markeredgecolor='none', color='green', alpha=0.05)\n",
    "        p2,=ax2.plot(X[0][centerIndex], X[1][centerIndex], '-',markeredgecolor='none', color='green', alpha=0.05)\n",
    "        \n",
    "        p3,=ax2.plot(x, y, 'o',markeredgecolor='none', color=colors[index], alpha=0.2)\n",
    "        p4,=ax2.plot(x, y, '-',markeredgecolor='none', color=colors[index], alpha=0.7)\n",
    "        \"\"\"if index==outlierIndices[0] and clusterType == 'KMeans':\n",
    "            p3,=ax2.plot(x, y, 'o',markeredgecolor='none', color='g', alpha=0.2)\n",
    "            p4,=ax2.plot(x, y, '-',markeredgecolor='none', color='g', alpha=0.7)\n",
    "        elif index in outlierIndices:\n",
    "            p3,=ax2.plot(x, y, 'o',markeredgecolor='none', color='r', alpha=0.2)\n",
    "            p4,=ax2.plot(x, y, '-',markeredgecolor='none', color='r', alpha=0.7)\n",
    "        else:\n",
    "            p3,=ax2.plot(x, y, 'o',markeredgecolor='none', color=color[index], alpha=0.2)\n",
    "            p4,=ax2.plot(x, y, '-',markeredgecolor='none', color=color[index], alpha=0.7)\"\"\"\n",
    "\n",
    "        \"\"\"plt.subplots_adjust(left=0.3)\n",
    "        plt.xlabel(r'${\\rm Time (Days)}$', fontsize=20)\n",
    "        plt.title(files[0],fontsize=20)\n",
    "        plt.ylabel(r'${\\rm \\Delta F/F}$', fontsize=20)\"\"\"\n",
    "        canvas.draw()\n",
    "        \n",
    "    def annotateCenter(XT, index):\n",
    "        \"\"\"Create popover label in 3d chart\n",
    "\n",
    "        Args:\n",
    "            X (np.array) - array of points, of shape (numPoints, 3)\n",
    "            index (int) - index (into points array X) of item which should be printed\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        x2, y2, _ = proj3d.proj_transform(XT[index][0], XT[index][1], XT[index][2], ax.get_proj())\n",
    "        # Either update the position, or create the annotation\n",
    "        if hasattr(annotateCenter, 'label'):\n",
    "            annotateCenter.label.xy = x2,y2\n",
    "            annotateCenter.label.update_positions(fig.canvas.renderer)\n",
    "        # Get data point from array of points X, at position index\n",
    "        else:\n",
    "            annotateCenter.label = ax.annotate( \"%s\" % files[index],\n",
    "                xy = (x2, y2), xytext = (40, 40), textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "                bbox = dict(boxstyle = 'round,pad=0.5', fc = 'g', alpha = 0.5),\n",
    "                arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "        \n",
    "        canvas.draw()\n",
    "    \n",
    "    \n",
    "    def onMouseClick(event, X):\n",
    "        \"\"\"Event that is triggered when mouse is clicked. Shows lightcurve for data point closest to mouse.\"\"\"\n",
    "        XT = np.array(reorganizeArray(X))\n",
    "        closestIndex = calcClosestDatapoint(XT, event)\n",
    "        drawData(lightcurveData, closestIndex)\n",
    "        \n",
    "    def onMouseRelease(event, X):\n",
    "        XT = np.array(reorganizeArray(X))\n",
    "        annotateCenter(XT,centerIndex)\n",
    "    \n",
    "    def connect(X):\n",
    "        \"\"\"\n",
    "        TODO: Limit it to being connected only when in axes.\n",
    "        ISSUE: Double connects when replotted. Doesn't seem to want to disconnect.\n",
    "        if str(ax)==str(event.inaxes):\n",
    "            if hasattr(connect,'cidpress'):\n",
    "                fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "            if hasattr(connect,'cidrelease'):\n",
    "                fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "            connect.cidpress = fig.canvas.mpl_connect('button_press_event', lambda event: onMouseClick(event,X))\n",
    "            connect.cidrelease = fig.canvas.mpl_connect('button_release_event', lambda event: onMouseRelease(event, X))\n",
    "        \"\"\"\n",
    "        if hasattr(connect,'cidpress'):\n",
    "            fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "        if hasattr(connect,'cidrelease'):\n",
    "            fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "        connect.cidpress = fig.canvas.mpl_connect('button_press_event', lambda event: onMouseClick(event,X))\n",
    "        connect.cidrelease = fig.canvas.mpl_connect('button_release_event', lambda event: onMouseRelease(event, X))\n",
    "\n",
    "    def disconnect(event):\n",
    "        if str(ax)==str(event.inaxes):\n",
    "            if hasattr(connect,'cidpress'):\n",
    "                fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "            if hasattr(connect,'cidrelease'):\n",
    "                fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "    \n",
    "    def redraw():       \n",
    "        # Clear the existing plots\n",
    "        ax.cla()\n",
    "        ax2.cla()        \n",
    "        if hasattr(annotateCenter, 'label'):\n",
    "            delattr(annotateCenter,'label')\n",
    "        # Get the new axes from the text input boxes\n",
    "        axis1 = int(e1.get())\n",
    "        axis2 = int(e2.get())\n",
    "        axis3 = int(e3.get())\n",
    "        # Get the data for the axes, assign data to the right axes\n",
    "        axesdict['xaxis'] = ffeatures[axis1]\n",
    "        axesdict['yaxis'] = ffeatures[axis2]\n",
    "        axesdict['zaxis'] = ffeatures[axis3]\n",
    "        # Get and assign labels to their axes\n",
    "        labellist[0] = betterlabels[axis1]\n",
    "        labellist[1] = betterlabels[axis2]\n",
    "        labellist[2] = betterlabels[axis3]\n",
    "        # Set those labels\n",
    "        ax.set_xlabel(labellist[0])\n",
    "        ax.set_ylabel(labellist[1])\n",
    "        ax.set_zlabel(labellist[2])\n",
    "        # Scatter the data\n",
    "        ax.scatter(axesdict['xaxis'], axesdict['yaxis'], axesdict['zaxis'],c=colors)\n",
    "        \n",
    "        currentData = [axesdict['xaxis'],axesdict['yaxis'],axesdict['zaxis']]\n",
    "\n",
    "        currentData1 = np.array(reorganizeArray(currentData))\n",
    "        annotateCenter(currentData1,centerIndex)\n",
    "        \n",
    "        if hasattr(redraw,'cidenter'):\n",
    "                fig.canvas.mpl_disconnect(redraw.cidenter)\n",
    "                fig.canvas.mpl_disconnect(redraw.cidexit)\n",
    "        connect(currentData)\n",
    "        #cidenter = fig.canvas.mpl_connect('axes_enter_event', lambda event: connect(event,currentData))\n",
    "        #cidexit = fig.canvas.mpl_connect('axes_leave_event', lambda event: disconnect(event))\n",
    "        canvas.draw()\n",
    "\n",
    "    redraw() # First draw\n",
    "    canvas.show()\n",
    "        \n",
    "    b1 = Tk.Button(master=root, text='Plot',command=redraw)\n",
    "    b1.pack(side=Tk.LEFT)\n",
    "    \n",
    "    def quit():\n",
    "        root.destroy()\n",
    "        sys.exit()\n",
    "        \n",
    "    Tk.Button(root, text=\"Quit\", command=quit).pack()\n",
    "    \n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The suggested 'heuristic' from the original DBSCAN paper to determine a ballpark for epsilon.\n",
    "Plots the distances from a center point (in this case we consider the center point of the clusters found through kmeans)\n",
    "sorted by distance. The 'elbow' gives the cutoff between cluster points and noise/outliers.\n",
    "\n",
    "I think this could just be applied directly to the kmeans clusters to determine the cutoff for each cluster. Though it's \n",
    "not obvious how to automate finding the bend.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\"\"\"--- Importing the relevant data ---\"\"\"\n",
    "identifier = raw_input('Enter the identifier of the data: ')\n",
    "dataID = 'input/'+str(identifier)+'dataByLightCurve.npy'\n",
    "data = np.load(dataID)\n",
    "\n",
    "\"\"\"for f in outlierfiles:\n",
    "    outlierIndices.append(files.index(f))\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # distance array containing all distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree').fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distArr = distances[:,3]\n",
    "    distArr.sort()\n",
    "    pts = [i for i in range(len(distArr))]\n",
    "\n",
    "    plt.scatter(pts, distArr)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN Clustering and Plotting\n",
    "Purpose: Find clusters and their outliers, output the results to np arrays and filelists\n",
    "TODO: output to np arrays\n",
    "\n",
    "Required:\n",
    ".npy file produced from keplerml.py. Data should be organized by light curve (rather than feature)\n",
    "\n",
    "This was created with the intent to cluster lightcurves based on the features calculated by keplerml.py. However,\n",
    "the methodology used is general and can be applied to any set of data as long as the data is saved as a numpy array.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pyfits\n",
    "import math\n",
    "# The order of the following is important, matplot.use('TkAgg') must be done before importing pyplot\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D,proj3d\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import Tkinter as Tk\n",
    "else:\n",
    "    import tkinter as Tk\n",
    "\n",
    "root = Tk.Tk()\n",
    "root.wm_title(\"Scatter\")\n",
    "import heapq\n",
    "import scipy.signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from numpy.random import RandomState\n",
    "from multiprocessing import Pool\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.widgets import RadioButtons\n",
    "import os\n",
    "import sys\n",
    "\n",
    "identifier = raw_input('Enter the identifier of the data or type \\'help\\' for clarification: ')\n",
    "if identifier == 'help':\n",
    "    n_help = 1\n",
    "    while(identifier == 'help'):\n",
    "        if n_help == 2:\n",
    "            print(\"\\'help\\' will be the assigned ID.\")\n",
    "        print(\n",
    "        '''\n",
    "        This will look for a numpy array (.npy) with the identifier as a prefix.\n",
    "        If you haven\\'t made the numpy array with keplerml.py, abort this with ctrl-c and run\n",
    "        keplerml.py. If you have run keplerml.py, input the identifier you chose.\n",
    "        \n",
    "        If you would like to assign \\'help\\' as the ID, enter it again when prompted,\n",
    "        this help text will not appear again.\n",
    "\n",
    "        For example, if you chose \\'ex\\', there will be a .npy file \n",
    "        called exdataByLightCurve.npy that will be read in.\n",
    "        For the identifier you would input \\'ex\\'\n",
    "        ''')\n",
    "        identifier = raw_input('Enter the identifier of the data: ')\n",
    "        \n",
    "        n_help += 1\n",
    "\n",
    "# Transposition. Will change an array orgnaized by data point (coordinates) to an array organized \n",
    "# by feature (list of all values for each feature) and vice versa.\n",
    "def reorganizeArray(X):\n",
    "    return [[X[i][j] for i in range(len(X))] for j in range(len(X[0]))]\n",
    "\n",
    "def DBSCAN_clusters(data,eps,min_points=None):\n",
    "    npdata = np.array(data)\n",
    "    if min_points==None:\n",
    "        est = DBSCAN(eps=eps)\n",
    "    else:\n",
    "        est = DBSCAN(eps=eps,min_samples=min_points)\n",
    "    \n",
    "    est.fit(npdata)\n",
    "    clusters = est.labels_\n",
    "    coreSampleIndices = est.core_sample_indices_\n",
    "    return clusters, coreSampleIndices\n",
    "\n",
    "def seperatedClusters(data,nclusters,clusters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    data - all data, organized by lightcurve \n",
    "    nclusters - number of clusters\n",
    "    clusters - array of cluster labels\n",
    "    \n",
    "    Purpose: Create arrays dataByCluster and clusterIndexes containing data seperated by\n",
    "    by cluster.\n",
    "    \"\"\"\n",
    "    dataByCluster = []\n",
    "    clusterIndexes = []\n",
    "    \n",
    "    # Will try to stick to the following:\n",
    "    # cluster i, lightcurve j, feature k\n",
    "    for i in range(nclusters):\n",
    "        # Keeping track of which points get pulled into each cluster:\n",
    "        clusterIndexes.append([j for j in range(len(data)) if clusters[j]==i])\n",
    "        # Separating the clusters out into their own arrays (w/in the cluster array)\n",
    "        dataByCluster.append([data[clusterIndexes[i][j]] for j in range(len(clusterIndexes[i]))])\n",
    "        \n",
    "        #Alternatively: (might have had some issues with the above, not sure...)\n",
    "        #dataByCluster.append([])\n",
    "        #for j in range(len(clusterIndexes[i])):\n",
    "        #    dataByCluster[i].append(data[clusterIndexes[i][j]])\n",
    "    return dataByCluster, clusterIndexes\n",
    "\n",
    "def makeClusterFilelists(clusterIndexes,files):\n",
    "    \"\"\"\n",
    "    returns: none\n",
    "    Makes filelists seperated by cluster.\n",
    "    \n",
    "    Args:\n",
    "    clusterIndexes - array containing n arrays each full of the indices of the \n",
    "    lightcurves for each cluster. The indices are the line number of the original\n",
    "    file list.\n",
    "    \n",
    "    files - array containing the files in the original filelist in the order of the\n",
    "    original file list.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(clusterIndexes)):\n",
    "        outputfile = open('clusterOutput/DBSCANc%sfilelist'%i,'w')\n",
    "        for j in range(len(clusterIndexes[i])):\n",
    "            outputfile.write('%s\\n'%files[clusterIndexes[i][j]]) \n",
    "\n",
    "def makeOutlierFilelists(allOutliers,files):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "    allOutliers - array of indices of outlying points\n",
    "    files - array of llc file names\n",
    "    \"\"\"\n",
    "    ncluster=1\n",
    "    for i in allOutliers:\n",
    "        with open('clusterOutput/DBSCANoutlierfilelist','a') as outputfile:\n",
    "            outputfile.write('%s\\n'%files[i])\n",
    "                \n",
    "def makeClusterNpy(dataByCluster):\n",
    "    n=1\n",
    "    for i in dataByCluster:\n",
    "        cluster = np.array(i)\n",
    "        np.save('clusterOutput/DBSCANdataByLightCurve'%n,cluster)\n",
    "        n+=1\n",
    "\n",
    "\n",
    "# tkinter only works in main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    dataID = 'input/'+str(identifier)+'dataByLightCurve.npy'\n",
    "    data = np.load(dataID)\n",
    "    ffeatures=[[data[i][j] for i in range(len(data))] for j in range(len(data[0]))]\n",
    "    filelist = 'input/'+str(identifier)+'filelist'\n",
    "    files = [line.strip() for line in open(filelist)]\n",
    "\n",
    "    clusterLabels,coreSampleIndices=DBSCAN_clusters(data,80)\n",
    "    \n",
    "    outlierInds = [i for i in range(len(clusterLabels))if clusterLabels[i]==-1]\n",
    "    outlierFiles = [files[i] for i in outlierInds]\n",
    "    \n",
    "    nclusters = max(clusterLabels)+1\n",
    "    \n",
    "    dataByCluster,clusterIndexes=seperatedClusters(data,nclusters,clusterLabels)\n",
    "    \n",
    "    makeClusterFilelists(clusterIndexes,files)\n",
    "    makeOutlierFilelists(outlierInds,files)\n",
    "    \"\"\"--- Organizing data and Labels ---\"\"\"\n",
    "    \n",
    "    ffeatures = reorganizeArray(data)\n",
    "\n",
    "    # Betterlabels will contain the titles of axes we'll actually want on there\n",
    "    listoffeatures = ['longtermtrend', 'meanmedrat', 'skews', 'varss', 'coeffvar', 'stds', 'numoutliers', 'numnegoutliers', 'numposoutliers', 'numout1s', 'kurt', 'mad', 'maxslope', 'minslope', 'meanpslope', 'meannslope', 'g_asymm', 'rough_g_asymm', 'diff_asymm', 'skewslope', 'varabsslope', 'varslope', 'meanabsslope', 'absmeansecder', 'num_pspikes', 'num_nspikes', 'num_psdspikes', 'num_nsdspikes','stdratio', 'pstrend', 'num_zcross', 'num_pm', 'len_nmax', 'len_nmin', 'mautocorrcoef', 'ptpslopes', 'periodicity', 'periodicityr', 'naiveperiod', 'maxvars', 'maxvarsr', 'oeratio', 'amp', 'normamp','mbp', 'mid20', 'mid35', 'mid50', 'mid65', 'mid80', 'percentamp', 'magratio', 'sautocorrcoef', 'autocorrcoef', 'flatmean', 'tflatmean', 'roundmean', 'troundmean', 'roundrat', 'flatrat']\n",
    "    \n",
    "    betterlabels = ['longtermtrend', 'meanmedrat', 'skews', 'varss', 'coeffvar', 'stds', 'numoutliers', 'numnegoutliers', 'numposoutliers', 'numout1s', 'kurt', 'mad', 'maxslope', 'minslope', 'meanpslope', 'meannslope', 'g_asymm', 'rough_g_asymm', 'diff_asymm', 'skewslope', 'varabsslope', 'varslope', 'meanabsslope', 'absmeansecder', 'num_pspikes', 'Number of Negative Spikes (Slope > 3*sigma)', 'num_psdspikes', 'num_nsdspikes','stdratio', 'pstrend', 'Number of Longterm Trendline Crossings', 'Number of Peaks', 'len_nmax', 'len_nmin', 'mautocorrcoef', 'ptpslopes', 'periodicity', 'periodicityr', 'naiveperiod', 'maxvars', 'maxvarsr', 'oeratio', 'amp', 'normamp','mbp', 'mid20', 'mid35', 'mid50', 'mid65', 'mid80', 'percentamp', 'magratio', 'sautocorrcoef', 'autocorrcoef', 'flatmean', 'tflatmean', 'roundmean', 'troundmean', 'roundrat', 'flatrat']\n",
    "    \n",
    "    # labellist keeps track of what each axis should be labelled\n",
    "    # initialized with some interesting axes (based on past experience)\n",
    "    labellist=[betterlabels[30],betterlabels[31],betterlabels[25]]\n",
    "    \n",
    "    # axesdict could probably be accomoplished with a list, see the new list currentData,\n",
    "    # I'm partial to the dictionary because it helps me keep things associated directly.\n",
    "    \n",
    "    axesdict = {'xaxis':ffeatures[30],'yaxis':ffeatures[31],'zaxis':ffeatures[25]}\n",
    "    fig = Figure()\n",
    "    \n",
    "    # a tk.DrawingArea\n",
    "    canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "    canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "    \n",
    "    # empty subplot for scattered data\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # tk text input boxes\n",
    "    e1 = Tk.Entry(master=root)\n",
    "    e1.insert(0,'30')\n",
    "    e1.pack(side=Tk.LEFT)\n",
    "    e2 = Tk.Entry(master=root)\n",
    "    e2.insert(0,'31')\n",
    "    e2.pack(side=Tk.LEFT)\n",
    "    e3 = Tk.Entry(master=root)\n",
    "    e3.insert(0,'25')\n",
    "    e3.pack(side=Tk.LEFT)\n",
    "    \n",
    "    def redraw():\n",
    "        # Clear the existing plots (if any)\n",
    "        ax.cla()\n",
    "        # Get the new axes from the text input boxes\n",
    "        axis1 = int(e1.get())\n",
    "        axis2 = int(e2.get())\n",
    "        axis3 = int(e3.get())\n",
    "        # Get the data for the axes, assign data to the right axes\n",
    "        axesdict['xaxis'] = ffeatures[axis1]\n",
    "        axesdict['yaxis'] = ffeatures[axis2]\n",
    "        axesdict['zaxis'] = ffeatures[axis3]\n",
    "        # Get and assign labels to their axes\n",
    "        labellist[0] = betterlabels[axis1]\n",
    "        labellist[1] = betterlabels[axis2]\n",
    "        labellist[2] = betterlabels[axis3]\n",
    "        # Set those labels\n",
    "        ax.set_xlabel(labellist[0])\n",
    "        ax.set_ylabel(labellist[1])\n",
    "        ax.set_zlabel(labellist[2])\n",
    "        # Scatter the data\n",
    "        ax.scatter(axesdict['xaxis'], axesdict['yaxis'], axesdict['zaxis'],c=clusterLabels.astype(np.float))\n",
    "        \n",
    "        canvas.draw()\n",
    "\n",
    "    redraw() # First draw\n",
    "    canvas.show()\n",
    "    \n",
    "    b1 = Tk.Button(master=root, text='Plot',command=redraw)\n",
    "    b1.pack(side=Tk.LEFT)\n",
    "    \n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Given a filelist this will generate a randomized filelist.\n",
    "User must input the number of sources they would like, and the path to the\n",
    "filelist from which they want to draw.\n",
    "This will generate a filelist named 'randomFilelist', will overwrite any previous random filelists\n",
    "by the same name.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "if sys.version_info[0] < 3:\n",
    "    from Tkinter import Tk\n",
    "else:\n",
    "    from tkinter import Tk\n",
    "from tkFileDialog import askopenfilename\n",
    "\n",
    "print('Select the filelist')\n",
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "filelist = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "\n",
    "NumLCs = raw_input('How many sources? ')\n",
    "\n",
    "files = [line.strip() for line in open(filelist)]\n",
    "\n",
    "randomLCs = np.random.choice(files, NumLCs, replace=False)\n",
    "\n",
    "# test if Tabby star is included in the otherwise random set, insert if not\n",
    "import fnmatch\n",
    "\n",
    "tabbyCheck = fnmatch.filter(randomLCs, '*8462852*')\n",
    "tabby = fnmatch.filter(files,'*8462852*')\n",
    "\n",
    "if tabbyCheck==[]:\n",
    "    randomLCs = np.delete(randomLCs,0)\n",
    "    randomLCs = np.insert(randomLCs,0,tabby)\n",
    "    \n",
    "# output random filelist to randomFilelist\n",
    "for i in range(len(randomLCs)):\n",
    "    outputfile = open('randomFilelist','w')\n",
    "    outputfile.write('%s\\n'%randomLCs[i]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
