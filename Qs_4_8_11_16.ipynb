{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2TkAgg\n",
    "from matplotlib import colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import Tkinter as Tk\n",
    "else:\n",
    "    import tkinter as Tk\n",
    "    \n",
    "#from tkFileDialog import askopenfilename,askdirectory,asksaveasfile\n",
    "sys.path.append('python')\n",
    "import clusterOutliers\n",
    "import keplerml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Existing Sample Data (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the FullSample is all common points between Quarters 4, 8, 11, and 16. These files are set up as pandas dataframes\n",
    "# and contain calculated features and previously computed cluster identifications.\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q4.dataSample = Q4.data\n",
    "Q4.filesSample =Q4.dataSample.index\n",
    "Q4.sampleGenerated = True\n",
    "Q4.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# This is only a sampling of the data, so the dataSample is, by definition, the data\n",
    "Q8.dataSample=Q8.data\n",
    "Q8.filesSample=Q8.dataSample.index\n",
    "# Specify that the sample is a good one\n",
    "Q8.sampleGenerated = True\n",
    "# Specifying that the sample has a TSNE reduction\n",
    "Q8.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q11.dataSample = Q11.data\n",
    "Q11.filesSample =Q11.dataSample.index\n",
    "Q11.sampleGenerated = True\n",
    "Q11.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q16.dataSample = Q16.data\n",
    "Q16.filesSample =Q16.dataSample.index\n",
    "Q16.sampleGenerated = True\n",
    "Q16.sampleTSNE = True\n",
    "\n",
    "Q_dict = {'Q8':Q8,'Q4':Q4,'Q11':Q11,'Q16':Q16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Q8', Index([u'db_out', u'pca_x', u'pca_y'], dtype='object'))\n",
      "('Q16', Index([u'db_out', u'pca_x', u'pca_y'], dtype='object'))\n",
      "('Q11', Index([u'db_out', u'pca_x', u'pca_y'], dtype='object'))\n",
      "('Q4', Index([u'db_out', u'pca_x', u'pca_y', u'db_cluster'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "for q in Q_dict:\n",
    "    print(q,Q_dict[q].data.columns[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q11pca = Q11.pca_fit()\n",
    "Q11.dataSample['pca_x']=q11pca.T[0]\n",
    "Q11.dataSample['pca_y']=q11pca.T[0]\n",
    "\n",
    "q16pca = Q16.pca_fit()\n",
    "Q16.dataSample['pca_x']=q16pca.T[0]\n",
    "Q16.dataSample['pca_y']=q16pca.T[0]\n",
    "\n",
    "Q11.save()\n",
    "Q16.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q11.save()\n",
    "Q16.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Want to create a common sample containing the outliers (outliers in any quarter) and 10k 'clustered' points\n",
    "\n",
    "# A dataframe index object containing all kID's for Q4 data ID'd as outliers\n",
    "\n",
    "out4 = np.array([i[:13] for i in Q4.data.index[Q4.data.db_out==-1]])\n",
    "out8 = np.array([i[:13] for i in Q8.data.index[Q8.data.db_out==-1]])\n",
    "out11 = np.array([i[:13] for i in Q11.data.index[Q11.data.db_out==-1]])\n",
    "out16 = np.array([i[:13] for i in Q16.data.index[Q16.data.db_out==-1]])\n",
    "# Outliers in any quarter, not necessarily in all quarters\n",
    "out_all = np.unique(np.concatenate([out4,out8,out11,out16]))\n",
    "\n",
    "kid_out4 = np.array([i[:13] for i in sample_4[sample_4.db_out==-1].index])   \n",
    "kid_out8 = np.array([i[:13] for i in sample_8[sample_8.db_out==-1].index])   \n",
    "kid_out11 = np.array([i[:13] for i in sample_11[sample_11.db_out==-1].index])   \n",
    "kid_out16 = np.array([i[:13] for i in sample_16[sample_16.db_out==-1].index])\n",
    "\n",
    "exc4 = np.setdiff1d(np.setdiff1d(np.setdiff1d(kid_out4,kid_out8),kid_out11),kid_out16)\n",
    "exc8 = np.setdiff1d(np.setdiff1d(np.setdiff1d(kid_out8,kid_out4),kid_out11),kid_out16)\n",
    "exc11 = np.setdiff1d(np.setdiff1d(np.setdiff1d(kid_out11,kid_out8),kid_out4),kid_out16)\n",
    "exc16 = np.setdiff1d(np.setdiff1d(np.setdiff1d(kid_out16,kid_out8),kid_out11),kid_out4)\n",
    "\n",
    "exc4_4 = sample_4[sample_4.index.str.contains('|'.join(exc4))]\n",
    "exc8_4 = sample_4[sample_4.index.str.contains('|'.join(exc8))]\n",
    "exc11_4 = sample_4[sample_4.index.str.contains('|'.join(exc11))]\n",
    "exc16_4 = sample_4[sample_4.index.str.contains('|'.join(exc16))]\n",
    "\n",
    "kid_exc4 = np.array([i[:13] for i in exc4.index])\n",
    "kid_exc8 = np.array([i[:13] for i in exc8.index])\n",
    "kid_exc11 = np.array([i[:13] for i in exc11.index])\n",
    "kid_exc16 = np.array([i[:13] for i in exc16.index])\n",
    "\n",
    "plt.scatter(sample_4.tsne_x,sample_4.tsne_y,c=sample_4.db_out)\n",
    "plt.scatter(exc4_4.tsne_x,exc4_4.tsne_y,c='blue',label=\"Q4 exclusive outliers\")\n",
    "plt.scatter(exc8_4.tsne_x,exc8_4.tsne_y,c='red',label=\"Q8 exclusive outliers\")\n",
    "plt.scatter(exc11_4.tsne_x,exc11_4.tsne_y,c='green',label=\"Q11 exclusive outliers\")\n",
    "plt.scatter(exc16_4.tsne_x,exc16_4.tsne_y,c='yellow',label=\"Q16 exclusive outliers\")\n",
    "plt.legend()\n",
    "\n",
    "plt.scatter(exc8_4.tsne_x,exc8_4.tsne_y,c='red')\n",
    "out4 = Q4.data[Q4.data.index.str.contains('|'.join(out_all))]\n",
    "out8 = Q8.data[Q8.data.index.str.contains('|'.join(out_all))]\n",
    "out11 = Q11.data[Q11.data.index.str.contains('|'.join(out_all))]\n",
    "out16 = Q16.data[Q16.data.index.str.contains('|'.join(out_all))]\n",
    "\n",
    "in4 = Q4.data[~Q4.data.index.str.contains('|'.join(out_all))].sample(10000)\n",
    "kid_in4 = np.array([i[:13] for i in in4.index])\n",
    "in8 = Q8.data[Q8.data.index.str.contains('|'.join(kid_in4))]\n",
    "in11 = Q11.data[Q11.data.index.str.contains('|'.join(kid_in4))]\n",
    "in16 = Q16.data[Q16.data.index.str.contains('|'.join(kid_in4))]\n",
    "\n",
    "sample_4 = pd.concat([out4,in4])\n",
    "sample_8 = pd.concat([out8,in8])\n",
    "sample_11 = pd.concat([out11,in11])\n",
    "sample_16 = pd.concat([out16,in16])\n",
    "\n",
    "sample_4.to_csv('paper_sample_4')\n",
    "sample_8.to_csv('paper_sample_8')\n",
    "sample_11.to_csv('paper_sample_11')\n",
    "sample_16.to_csv('paper_sample_16')\n",
    "\n",
    "plt.scatter(sample_4.tsne_x,sample_4.tsne_y,c=sample_4.db_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def scale_sample(df):\n",
    "    data = df.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=df.columns[0:60],index = df           .index,data=scaledData)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "def cluster_sample(q):\n",
    "    \n",
    "    Qob = Q_dict[q]\n",
    "    Qdf = Qob.dataSample\n",
    "    df_scaled = scale_sample(Qdf)\n",
    "    dbout = Qob.db_out(df_scaled,check_tabby=True)\n",
    "    \n",
    "    return dbout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "Sampling data for parameter estimation...\n",
      "Calculating nearest neighbor distances...\n",
      "Finding elbow...\n",
      "\n",
      "        Epsilon is in the neighborhood of 04.09.\n",
      "        \n",
      "Scaling density...\n",
      "Clustering data with DBSCAN, eps=04.09,min_samples=56...\n",
      "Tabby has been found to be an outlier in DBSCAN.\n",
      "There were 2 clusters and 5041 total outliers\n",
      "0:14:45.992268\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "Q8.dataSample['db_out'] = cluster_sample('Q8')\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:21:49.619896\n",
      "5032\n"
     ]
    }
   ],
   "source": [
    "df = scale_sample(Q8.dataSample)\n",
    "start = datetime.now()\n",
    "est = DBSCAN(eps=4.09,min_samples=56,algorithm='ball_tree',n_jobs=-1)\n",
    "est.fit(df)\n",
    "print(datetime.now()-start)\n",
    "print(len(est.labels_[est.labels_==-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "def near_neighbors(q):\n",
    "    Qob = Q_dict[q]\n",
    "    Q = Qob.dataSample\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree',n_jobs=-1).fit(df_scaled)\n",
    "    distances, indices = nbrs.kneighbors(df_scaled)\n",
    "    return distances, indices\n",
    "\n",
    "distances, indices = near_neighbors('Q8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eps_est(data,n=4,verbose=True):\n",
    "    # distance array containing all distances\n",
    "    if verbose:print(\"Calculating nearest neighbor distances...\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=max(n+1,100), algorithm='ball_tree',n_jobs=-1).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    del nbrs\n",
    "    distArr = distances[:,n]\n",
    "    distArr.sort()\n",
    "    del distances\n",
    "    pts = range(len(distArr))\n",
    "\n",
    "    # The following looks for the first instance (past the mid point)\n",
    "    # where the mean of the following [number] points\n",
    "    # is at least (cutoff-1)*100% greater than the mean of the previous [number] points.\n",
    "    \n",
    "    number = int(np.ceil(len(data)/500))\n",
    "    cutoff = 1.05\n",
    "    if verbose:print(\"Finding elbow...\")\n",
    "    \n",
    "    for i in range(int(np.ceil(len(pts)/2)),len(pts)-number):\n",
    "        if np.mean(distArr[i+1:i+number])>=cutoff*np.mean(distArr[i-number:i-1]):\n",
    "            dbEps = distArr[i]\n",
    "            pt=pts[i]\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\"\"\n",
    "        Epsilon is in the neighborhood of {:05.2f}.\n",
    "        \"\"\".format(dbEps))\n",
    "    return dbEps,distArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbscan_w_outliers(data,min_n=4,check_tabby=False,verbose=True):\n",
    "    min_n=int(np.ceil(len(data)/50))\n",
    "    # numpy array of dataframe for fit later\n",
    "    X=np.array([np.array(data.loc[i]) for i in data.index])\n",
    "    if verbose:print(\"Estimating Parameters...\")\n",
    "    if len(X)>10000:\n",
    "        # Large datasets have presented issues where a single high density cluster \n",
    "        # leads to an epsilon of 0.0 for 4 neighbors.\n",
    "        # We adjust for this by calculating epsilon with 4 neighbors\n",
    "        # for a sample of the data, then we scale min_neighbors accordingly.\n",
    "        if verbose:print(\"Sampling data for parameter estimation...\")\n",
    "        X_sample = data.sample(n=10000)\n",
    "    else:\n",
    "        X_sample = data\n",
    "    dbEps,distArr = eps_est(X_sample,n=min_n,verbose=verbose)\n",
    "    \n",
    "    if verbose:print(\"Clustering data with DBSCAN, eps={:05.2f},min_samples={}...\".format(dbEps,min_n))\n",
    "    est = DBSCAN(eps=dbEps,min_samples=min_n,n_jobs=-1)\n",
    "    est.fit(X)\n",
    "    clusterLabels = est.labels_\n",
    "    # Outlier score: distance to 4th neighbor?\n",
    "\n",
    "    if check_tabby:\n",
    "        if data.index.str.contains('8462852').any():\n",
    "            tabbyInd = list(data.index).index(data[data.index.str.contains('8462852')].index[0])\n",
    "            if clusterLabels[tabbyInd] == -1:\n",
    "                print(\"Tabby has been found to be an outlier in DBSCAN.\")\n",
    "            else:\n",
    "                print(\"Tabby has NOT been found to be an outlier in DBSCAN\")\n",
    "        else:\n",
    "            print(\"MISSING: Tabby is not in this data.\")\n",
    "                     \n",
    "    numout = len(clusterLabels[clusterLabels==-1])\n",
    "    numclusters = max(clusterLabels+1)\n",
    "    if verbose:\n",
    "        if numclusters==1:\n",
    "            print(\"There was %s cluster and %s total outliers\"%(numclusters,numout))\n",
    "        else:\n",
    "            print(\"There were %s clusters and %s total outliers\"%(numclusters,numout))\n",
    "\n",
    "    return clusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "        Epsilon is in the neighborhood of 21.763520029.\n",
      "        \n",
      "Clustering data with DBSCAN...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "def cluster_sample(q):\n",
    "    \n",
    "    Qob = Q_dict[q]\n",
    "    Q = Qob.dataSample\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "    \n",
    "    \n",
    "    return Qob.db_out(df_scaled,check_tabby=True)\n",
    "\n",
    "start = datetime.now()\n",
    "Q4.dataSample['db_out'] = cluster_sample('Q4')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q8.dataSample['db_out'] = cluster_sample('Q8')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q11.dataSample['db_out'] = cluster_sample('Q11')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q16.dataSample['db_out'] = cluster_sample('Q16')\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for q in Q_dict:\n",
    "    pca = Q_dict[q].pca_fit(Q_dict[q].dataSample.iloc[:,0:60])\n",
    "    Q_dict[q].dataSample['pca_x'] = pca.T[0]\n",
    "    Q_dict[q].dataSample['pca_y'] = pca.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 6.94193875491, \n",
      "    with an average of 790.0 neighbors within epsilon,\n",
      "    395.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "Tabby has not Not found to be an outlier in DBSCAN\n",
      "There were 1 clusters and 213 total outliers\n",
      "0:00:06.940155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 7.10991455506, \n",
      "    with an average of 1121.0 neighbors within epsilon,\n",
      "    560.5 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "Tabby has not Not found to be an outlier in DBSCAN\n",
      "There were 1 clusters and 236 total outliers\n",
      "0:00:13.435256\n",
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 6.12068023561, \n",
      "    with an average of 880.0 neighbors within epsilon,\n",
      "    440.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "There were 1 clusters and 372 total outliers\n",
      "0:00:07.660973\n",
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 5.96482020724, \n",
      "    with an average of 810.0 neighbors within epsilon,\n",
      "    405.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "There were 1 clusters and 356 total outliers\n",
      "0:00:06.505793\n"
     ]
    }
   ],
   "source": [
    "def cluster_sample(q):\n",
    "    start = datetime.now()\n",
    "    Q = q.outliers\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "    db_labels = q.db_out(df_scaled)\n",
    "    print(datetime.now()-start)\n",
    "\n",
    "    return db_labels\n",
    "\n",
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers = Q.dataSample[Q.dataSample.db_out == -1]\n",
    "    Q.outliers['out_out'] = cluster_sample(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             tsne_x  tsne_y\n",
      "cluster                \n",
      "-1           16      16\n",
      " 0         3841    3841\n",
      " 1           25      25\n",
      " 2            4       4\n",
      " 3           16      16\n",
      " 4           14      14\n",
      " 5           13      13\n",
      " 6           13      13\n",
      " 7            4       4\n",
      " 8            3       3        \n",
      "                tsne_x     tsne_y\n",
      "cluster                      \n",
      "-1       22.819558  10.467281\n",
      " 0        4.037788   4.507165\n",
      " 1        0.217006   0.382377\n",
      " 2        0.050079   0.056157\n",
      " 3        0.123062   0.105542\n",
      " 4        0.095739   0.132884\n",
      " 5        0.052605   0.014126\n",
      " 6        0.032419   0.007703\n",
      " 7        0.035876   0.015066\n",
      " 8        0.030285   0.283041\n",
      "        \n",
      "Plotting.\n",
      "window closed.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import pyfits\n",
    "from matplotlib.widgets import Slider\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import km_outliers\n",
    "root = Tk.Tk()\n",
    "root.wm_title(\"Scatter\")\n",
    "from IPython.display import clear_output\n",
    "from accelerate.cuda import cuda\n",
    "\n",
    "\"\"\"--- import light curve data ---\"\"\"\n",
    "\n",
    "Qdict = {'Q4':Q4,'Q8':Q8,'Q11':Q11,'Q16':Q16}\n",
    "Q = Qdict['Q8']\n",
    "Qdf = Q.data\n",
    "pathtofits = Q.fitsDir\n",
    "files = Qdf.index\n",
    "clusterLabels=Qdf.db_out\n",
    "\n",
    "\n",
    "cmap = matplotlib.cm.ScalarMappable(\\\n",
    "    norm=matplotlib.colors.Normalize(vmin=min(clusterLabels),vmax=max(clusterLabels)),\\\n",
    "    cmap='viridis')\n",
    "# data is an array containing each data point\n",
    "data = np.array(Qdf[['tsne_x','tsne_y']])\n",
    "\n",
    "# tsneX has all the x-coordinates\n",
    "tsneX = Qdf.tsne_x\n",
    "# tsneY has all the y-coordinates\n",
    "tsneY = Qdf.tsne_y\n",
    "\n",
    "data_out = Qdf[clusterLabels==-1]\n",
    "outX = data_out.tsne_x\n",
    "outY = data_out.tsne_y\n",
    "files_out = data_out.index\n",
    "\n",
    "data_cluster = Qdf[clusterLabels!=-1]\n",
    "clusterX = data_cluster.tsne_x\n",
    "clusterY = data_cluster.tsne_y\n",
    "files_cluster = data_cluster.index\n",
    "\n",
    "\n",
    "\"\"\"--- Organizing data and Labels ---\"\"\"\n",
    "\n",
    "\n",
    "if Qdf.index.str.contains('8462852').any():\n",
    "    tabbyInd = list(Qdf.index).index(Qdf[Qdf.index.str.contains('8462852')].index[0])            \n",
    "else:\n",
    "    tabbyInd = 0\n",
    "fig = Figure(figsize=(20,10))\n",
    "\n",
    "# a tk.DrawingArea\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "# Toolbar to help navigate the data (pan, zoom, save image, etc.)\n",
    "toolbar = NavigationToolbar2TkAgg(canvas, root)\n",
    "toolbar.update()\n",
    "canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "\n",
    "gs = gridspec.GridSpec(22,6)\n",
    "\n",
    "def eps_est(data,n):\n",
    "    \n",
    "    # distance array containing all distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=int(np.ceil(.1*len(data))), algorithm='ball_tree',n_jobs=-1).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    # Distance to 2*N/100th instead of 4th because: ... reasons\n",
    "    neighbors = n\n",
    "    distArr = distances[:,neighbors]\n",
    "    distArr.sort()\n",
    "    pts = range(len(distArr))\n",
    "\n",
    "    # The following looks for the first instance (past the mid point)\n",
    "    # where the mean of the following [number] points\n",
    "    # is at least (cutoff-1)*100% greater than the mean of the previous [number] points.\n",
    "    \n",
    "    number = 5\n",
    "    cutoff = 1.05\n",
    "    for i in range(int(np.ceil(len(pts)/2)),len(pts)-number):\n",
    "        if np.mean(distArr[i+1:i+number])>=cutoff*np.mean(distArr[i-number:i-1]):\n",
    "            dbEps = distArr[i]\n",
    "            pt=pts[i]\n",
    "            break\n",
    "            \n",
    "    return dbEps\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    # empty subplot for scattered data\n",
    "    ax = fig.add_subplot(gs[0:7,:4])\n",
    "    # empty subplot for lightcurves\n",
    "    ax2 = fig.add_subplot(gs[9:18,:])\n",
    "    # empty subplot for center detail\n",
    "    ax3 = fig.add_subplot(gs[0:7,4:])\n",
    "    \n",
    "    min_points = 4\n",
    "    eps = eps_est(data,min_points)\n",
    "    ax_s_min_n = fig.add_subplot(gs[20,:])\n",
    "    ax_s_eps = fig.add_subplot(gs[21,:])\n",
    "    s_neighbors = Slider(ax_s_min_n,'Neighbors',1,50,valinit=min_points,valfmt='%2i')\n",
    "    s_eps = Slider(ax_s_eps,'Epsilon',.01,10,valinit=eps,valfmt='%8.3f')\n",
    "    \n",
    "# Set those labels\n",
    "ax.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "ax.set_ylabel(\"Reduced Y\",fontsize=18)\n",
    "\n",
    "ax3.set_title(\"Center Density Detail\")\n",
    "ax3.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "ax3.set_ylabel(\"Reduced Y\",fontsize=18)    \n",
    "\n",
    "def read_kepler_curve(file):\n",
    "    \"\"\"\n",
    "    Given the path of a fits file, this will extract the light curve and normalize it.\n",
    "    \"\"\"\n",
    "\n",
    "    lc = pyfits.getdata(file)\n",
    "    t = lc.field('TIME')\n",
    "    f = lc.field('PDCSAP_FLUX')\n",
    "    err = lc.field('PDCSAP_FLUX_ERR')\n",
    "\n",
    "    err = err[np.isfinite(t)]\n",
    "    f = f[np.isfinite(t)]\n",
    "    t = t[np.isfinite(t)]\n",
    "    err = err[np.isfinite(f)]\n",
    "    t = t[np.isfinite(f)]\n",
    "    f = f[np.isfinite(f)]\n",
    "    err = err/np.median(f)\n",
    "    nf = f / np.median(f)\n",
    "\n",
    "    return t, nf, err\n",
    "    \n",
    "@cuda.jit\n",
    "def distance_cuda(dx,dy,dd):\n",
    "    bx = cuda.blockIdx.x # which block in the grid?\n",
    "    bw = cuda.blockDim.x # what is the size of a block?\n",
    "    tx = cuda.threadIdx.x # unique thread ID within a blcok\n",
    "    i = tx + bx * bw + 1\n",
    "    if i>len(dd):\n",
    "        return\n",
    "\n",
    "    # d_ph is a distance placeholder\n",
    "    d_ph = (dx[i]-dx[0])**2+(dy[i]-dy[0])**2\n",
    "\n",
    "    dd[i]=d_ph**.5\n",
    "    return\n",
    "\n",
    "def distances(pts,ex,ey):\n",
    "    # Calculates distances between N points\n",
    "    pts = np.array(pts)\n",
    "    N=len(pts)\n",
    "\n",
    "    # Allocate host memory arrays\n",
    "    # Transpose pts array to n_dims x n_pts, each index of x contains all of a dimensions coordinates\n",
    "    XT = np.transpose(pts)\n",
    "    x = np.array(XT[0])\n",
    "    x = np.insert(x,0,ex)\n",
    "    y = np.array(XT[1])\n",
    "    y = np.insert(y,0,ey)\n",
    "    d = np.zeros(N+1)\n",
    "\n",
    "\n",
    "    # Allocate and copy GPU/device memory\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_d = cuda.to_device(d)\n",
    "\n",
    "    threads_per_block = 128\n",
    "    number_of_blocks =N/128+1 \n",
    "\n",
    "    distance_cuda [ number_of_blocks, threads_per_block ] (d_x,d_y,d_d)\n",
    "\n",
    "    d_d.copy_to_host(d)\n",
    "    return d[1:]   \n",
    "\n",
    "def calcClosestDatapoint(X, event):\n",
    "    \"\"\"Calculate which data point is closest to the mouse position.\n",
    "\n",
    "    Args:\n",
    "        X (np.array) - array of points, of shape (numPoints, 2)\n",
    "        event (MouseEvent) - mouse event (containing mouse position)\n",
    "    Returns:\n",
    "        smallestIndex (int) - the index (into the array of points X) of the element closest to the mouse position\n",
    "    \"\"\"\n",
    "    ex,ey = ax.transData.inverted().transform((event.x,event.y))\n",
    "\n",
    "    #distances = [distance (XT[:,i], event) for i in range(XT.shape[1])]\n",
    "    Ds = distances(X,ex,ey)\n",
    "    return np.argmin(Ds)\n",
    "\n",
    "def drawData(index):\n",
    "    # Plots the lightcurve of the point chosen\n",
    "    ax2.cla()\n",
    "    f = pathtofits+Qdf.index[index]\n",
    "    t,nf,err=read_kepler_curve(f)\n",
    "\n",
    "    x=t\n",
    "    y=nf\n",
    "\n",
    "    axrange=0.55*(max(y)-min(y))\n",
    "    mid=(max(y)+min(y))/2\n",
    "    yaxmin = mid-axrange\n",
    "    yaxmax = mid+axrange\n",
    "    if yaxmin < .95:\n",
    "        if yaxmax > 1.05:\n",
    "            ax2.set_ylim(yaxmin,yaxmax)\n",
    "        else:\n",
    "            ax2.set_ylim(yaxmin,1.05)\n",
    "    elif yaxmax > 1.05:\n",
    "        ax2.set_ylim(.95,yaxmax)\n",
    "    else:\n",
    "        ax2.set_ylim(.95,1.05)\n",
    "\n",
    "    if clusterLabels[index]==-1:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = cmap.to_rgba(clusterLabels[index])\n",
    "        \n",
    "    ax2.plot(x, y, 'o',markeredgecolor='none', c=color, alpha=0.2)\n",
    "    ax2.plot(x, y, '-',markeredgecolor='none', c=color, alpha=0.7)\n",
    "    #ax2.set_title(files[index][:13],fontsize = 20)\n",
    "    ax2.set_xlabel('Time (Days)',fontsize=22)\n",
    "    ax2.set_ylabel(r'$\\frac{\\Delta F}{F}$',fontsize=30)\n",
    "\n",
    "    fig.suptitle(files[index][:13],fontsize=30)\n",
    "\n",
    "    canvas.draw()\n",
    "\n",
    "def annotatePt(XT, index,ignore=False):\n",
    "    \"\"\"Create popover label in 3d chart\n",
    "\n",
    "    Args:\n",
    "        X (np.array) - array of points, of shape (numPoints, 3)\n",
    "        index (int) - index (into points array X) of item which should be printed\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    x2, y2 = XT[index][0], XT[index][1]\n",
    "    # Either update the position, or create the annotation\n",
    "    if not ignore:\n",
    "        if hasattr(annotatePt, 'label'):\n",
    "            annotatePt.label.remove()\n",
    "            annotatePt.emph.remove()\n",
    "        if hasattr(annotatePt, 'emphCD'):\n",
    "            annotatePt.emphCD.remove()\n",
    "    \n",
    "    x_ax = ax.get_xlim()[1]-ax.get_xlim()[0]\n",
    "    y_ax = ax.get_ylim()[1]-ax.get_ylim()[0]\n",
    "    # Get data point from array of points X, at position index\n",
    "    annotatePt.label = ax.annotate( \"\",\n",
    "        xy = (x2, y2), xytext = (x2+.1*x_ax, y2+.2*y_ax),\n",
    "        arrowprops = dict(headlength=20,headwidth=20,width=6,shrink=.1,color='red'))\n",
    "    annotatePt.emph = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "    if files[index] in files_cluster:\n",
    "        annotatePt.emphCD = ax3.scatter(x2,y2,marker='o',s=150,c='red')\n",
    "    else:\n",
    "        annotatePt.emphCD = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "    canvas.draw()\n",
    "\n",
    "\n",
    "def onMouseClick(event, X):\n",
    "    \"\"\"Event that is triggered when mouse is clicked. Shows lightcurve for data point closest to mouse.\"\"\"\n",
    "    #XT = np.array(X.T) # array organized by feature, each in it's own array\n",
    "    closestIndex = calcClosestDatapoint(X, event)\n",
    "    drawData(closestIndex)\n",
    "\n",
    "def onMouseRelease(event, X):\n",
    "    #XT = np.array(X.T)\n",
    "    closestIndex = calcClosestDatapoint(X, event)\n",
    "    annotatePt(X,closestIndex)\n",
    "    #for centerIndex in centerIndices:\n",
    "    #    annotateCenter(XT,centerIndex)\n",
    "\n",
    "def connect(X):\n",
    "    if hasattr(connect,'cidpress'):\n",
    "        fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "    if hasattr(connect,'cidrelease'):\n",
    "        fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "    \n",
    "    connect.cidpress = fig.canvas.mpl_connect('button_press_event', lambda event: onMouseClick(event,X))\n",
    "    connect.cidrelease = fig.canvas.mpl_connect('button_release_event', lambda event: onMouseRelease(event, X))\n",
    "    \n",
    "def enter_axes(event):\n",
    "    if event.inaxes == ax:\n",
    "        connect(data)\n",
    "\n",
    "def leave_axes(event):\n",
    "    if event.inaxes == ax:\n",
    "        fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "        fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "\n",
    "def replot():\n",
    "    ax.cla()\n",
    "    ax3.cla()\n",
    "    global cmap\n",
    "    cmap = matplotlib.cm.ScalarMappable(\\\n",
    "            norm=matplotlib.colors.Normalize(vmin=0,vmax=max(clusterLabels)),\\\n",
    "            cmap='viridis')\n",
    "    data_out = Qdf[clusterLabels==-1]\n",
    "    outX = data_out.tsne_x\n",
    "    outY = data_out.tsne_y\n",
    "    # Set those labels\n",
    "    ax.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "    ax.set_ylabel(\"Reduced Y\",fontsize=18)\n",
    "    # Scatter the data\n",
    "    ax.scatter(Qdf[clusterLabels!=-1].tsne_x,Qdf[clusterLabels!=-1].tsne_y,\n",
    "               c=clusterLabels[clusterLabels!=-1],cmap=\"viridis\",alpha=.5)\n",
    "    ax.scatter(outX,outY,c='red',s=30,alpha=.8)\n",
    "    ax.annotate(\"Outliers: %6i\"%len(clusterLabels[clusterLabels==-1]),\n",
    "               xy=(0,0),textcoords=\"axes fraction\",xytext=(0.05,.9))\n",
    "    ax.annotate(\"Clusters: %6i\"%(max(clusterLabels)+1),\n",
    "               xy=(0,0),textcoords=\"axes fraction\",xytext=(0.05,.85))\n",
    "\n",
    "    ax3.scatter(Qdf[clusterLabels!=-1].tsne_x,Qdf[clusterLabels!=-1].tsne_y,\n",
    "            c=clusterLabels[clusterLabels!=-1],cmap=\"viridis\",alpha=.2)\n",
    "    annotatePt(data,tabbyInd,ignore=True)\n",
    "    \n",
    "def recluster_km(X_scaled,k):\n",
    "\n",
    "    global clusterLabels\n",
    "    clusterLabels = km_outliers.kmeans_w_outliers(X_scaled,min_samples)\n",
    "    replot()\n",
    "    \n",
    "    clear_output()\n",
    "    df = pd.DataFrame({'tsne_x':X_scaled.tsne_x,\n",
    "                       'tsne_y':X_scaled.tsne_y,\n",
    "                       'cluster':clusterLabels})\n",
    "    \n",
    "    for i in range(max(clusterLabels)+1):\n",
    "        center = [sum(Qdf[clusterLabels==i].tsne_x)/len(Qdf[clusterLabels==i]),\n",
    "                  sum(Qdf[clusterLabels==i].tsne_y)/len(Qdf[clusterLabels==i])]\n",
    "        distFromCenter = [sum((pt-center)**2)**.5 for pt in np.array(Qdf[clusterLabels==i][['tsne_x','tsne_y']])]\n",
    "        sigma = np.std(distFromCenter)\n",
    "        cutoff = 4*sigma\n",
    "        circle = plt.Circle((center[0],center[1]),cutoff,edgecolor=None,alpha=.2)\n",
    "        ax.add_artist(circle)\n",
    "    \n",
    "    return\n",
    "\n",
    "def recluster_db(X_scaled,eps,min_samples):\n",
    "    ax.cla()\n",
    "    ax3.cla()\n",
    "    est = DBSCAN(eps=eps,min_samples=min_samples)\n",
    "    est.fit(X_scaled)\n",
    "\n",
    "    global clusterLabels\n",
    "    clusterLabels = est.labels_\n",
    "    \n",
    "    clear_output()\n",
    "    df = pd.DataFrame({'tsne_x':X_scaled.tsne_x,\n",
    "                       'tsne_y':X_scaled.tsne_y,\n",
    "                       'cluster':clusterLabels})\n",
    "    gb = df.groupby('cluster')\n",
    "    print(\"\"\"\n",
    "    %s        \n",
    "    %s\n",
    "        \"\"\"%(gb.count(),gb.std()))\n",
    "    replot()\n",
    "    return\n",
    "\n",
    "def update_eps(val):\n",
    "    eps = s_eps.val\n",
    "    min_samples = int(s_neighbors.val)\n",
    "\n",
    "    #X = Qdf.iloc[:,0:60]\n",
    "    #scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #scaledData = scaler.transform(X)\n",
    "    #X_scaled = pd.DataFrame(columns=X.columns[0:60],index = X.index,data=scaledData)\n",
    "    \n",
    "    X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "    recluster_db(X_scaled,eps,min_samples)\n",
    "    return\n",
    "\n",
    "    \n",
    "def update_neighbors(val):\n",
    "    min_samples = int(s_neighbors.val)\n",
    "    \n",
    "    #X = Qdf.iloc[:,0:60]\n",
    "    #scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #scaledData = scaler.transform(X)\n",
    "    #X_scaled = pd.DataFrame(columns=X.columns[0:60],index = X.index,data=scaledData)\n",
    "    \n",
    "    X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "    # Automatically determines a suggestion for epsilon based on the number of neighbors chosen\n",
    "    eps = eps_est(X_scaled,min_samples)\n",
    "    # the following changes eps and initiates update_eps\n",
    "    s_eps.set_val(eps)\n",
    "    \n",
    "    return\n",
    "        \n",
    "fig.canvas.mpl_connect('axes_enter_event',enter_axes)\n",
    "fig.canvas.mpl_connect('axes_leave_event',leave_axes)\n",
    "s_eps.on_changed(update_eps)\n",
    "s_neighbors.on_changed(update_neighbors)\n",
    "s_neighbors.set_val(4)\n",
    "annotatePt(data,tabbyInd)\n",
    "drawData(tabbyInd)\n",
    "\n",
    "canvas.draw()\n",
    "canvas.show()\n",
    "print(\"Plotting.\")         \n",
    "\n",
    "def _delete_window():\n",
    "    print(\"window closed.\")\n",
    "    root.destroy()\n",
    "    sys.exit()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\",_delete_window)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Former code for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the features created with keplerml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User defined\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "\n",
    "#Generating an initial sample of 10,000 light curves\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "#Q4_Sample = Q4.randSampleWTabby(10000)\n",
    "\n",
    "\"\"\"Q4.sample_tsne_fit()\n",
    "print(\"K-means\")\n",
    "Q4.sample_km_out()\n",
    "print(\"DBSCAN\")\n",
    "Q4.sample_db_out()\"\"\"\n",
    "# The Kepler IDs of each lc, without the timestamp, to identify in other quarters\n",
    "kID = [i[:13] for i in Q4.data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports output from Q's 4, 8, 11, and 16 and finds all lc's in all Q's\n",
    "#\"\"\"\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# The Kepler IDs of each lc, without the timestamp, to identify in other quarters\n",
    "kID = [i[:13] for i in Q4.data.index]\n",
    "# \"\"\"\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# Creating a data sample set using the kIDs from the original quarter\n",
    "Q8_common = Q8.data[Q8.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q8_common.index]\n",
    "print(\"Q8 imported.\")\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q11_common = Q11.data[Q11.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q11_common.index]\n",
    "print(\"Q11 imported.\")\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q16_common = Q16.data[Q16.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q16_common.index]\n",
    "print(\"Q16 imported.\")\n",
    "# Ensure all quarters contain the same data\n",
    "Q4_common = Q4.data[Q4.data.index.str.contains('|'.join(kID))]\n",
    "Q8_common = Q8_common[Q8_common.index.str.contains('|'.join(kID))]\n",
    "Q11_common = Q11_common[Q11_common.index.str.contains('|'.join(kID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q4.dataSample = Q4_common\n",
    "Q4.filesSample = Q4.dataSample.index\n",
    "Q4.sampleGenerated = True\n",
    "\n",
    "Q8.dataSample = Q8_common\n",
    "Q8.filesSample = Q8.dataSample.index\n",
    "Q8.sampleGenerated = True\n",
    "\n",
    "Q11.dataSample = Q11_common\n",
    "Q11.filesSample = Q11.dataSample.index\n",
    "Q11.sampleGenerated = True\n",
    "\n",
    "Q16.dataSample = Q16_common\n",
    "Q16.filesSample = Q16.dataSample.index\n",
    "Q16.sampleGenerated = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further down the rabbit hole..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1183 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 3949 / 3949\n",
      "[t-SNE] Mean sigma: 2.278248\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.300924\n",
      "[t-SNE] Error after 100 iterations: 0.300924\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 1681 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 5605 / 5605\n",
      "[t-SNE] Mean sigma: 2.112685\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 0.210839\n",
      "[t-SNE] Error after 125 iterations: 0.210839\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1318 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 4396 / 4396\n",
      "[t-SNE] Mean sigma: 2.159918\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.259040\n",
      "[t-SNE] Error after 100 iterations: 0.259040\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1213 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 4049 / 4049\n",
      "[t-SNE] Mean sigma: 2.169621\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.286553\n",
      "[t-SNE] Error after 100 iterations: 0.286553\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    tsne_fit = Q.tsne_fit(Q.outliers.iloc[:,0:60])\n",
    "    Q.outliers['tsne_x'] = tsne_fit.T[0]\n",
    "    Q.outliers['tsne_y'] = tsne_fit.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 22 outliers in 1 clusters\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1207 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 4027 / 4027\n",
      "[t-SNE] Mean sigma: 2.265101\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 0.267778\n",
      "[t-SNE] Error after 125 iterations: 0.267778\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "Q = Qdict['Q4']\n",
    "Qdf = Q.outliers\n",
    "X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "Qdf.cluster = km_outliers.kmeans_w_outliers(X_scaled,1)\n",
    "Q_rh = Qdf[Qdf.cluster==0]\n",
    "tsne_fit = Q.tsne_fit(Q_rh.iloc[:,0:60])\n",
    "Q_rh['tsne_x'] = tsne_fit.T[0]\n",
    "Q_rh['tsne_y'] = tsne_fit.T[1]\n",
    "plt.scatter(Q_rh.tsne_x,Q_rh.tsne_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to outliers csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers.to_csv(\"/home/dgiles/Documents/KeplerLCs/output/\"+q+\"_outliers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outliers reclustered, re-tsne'd\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "\n",
    "Q_dict = {'Q8':Q8,'Q4':Q4,'Q11':Q11,'Q16':Q16}\n",
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers = Q.data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
