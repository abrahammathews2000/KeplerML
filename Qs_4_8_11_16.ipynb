{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode EXPIRED 116 days ago\n",
      "\n",
      "    Your usage of mkl is now out of compliance with the license agreement.\n",
      "    A license for mkl can be purchased at: http://continuum.io\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2TkAgg\n",
    "from matplotlib import colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import Tkinter as Tk\n",
    "else:\n",
    "    import tkinter as Tk\n",
    "    \n",
    "from tkFileDialog import askopenfilename,askdirectory,asksaveasfile\n",
    "sys.path.append('python')\n",
    "import clusterOutliers\n",
    "import keplerml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Existing Sample Data (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the FullSample is all common points between Quarters 4, 8, 11, and 16. These files are set up as pandas dataframes\n",
    "# and contain calculated features and previously computed cluster identifications.\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q4.dataSample = Q4.data\n",
    "Q4.filesSample =Q4.dataSample.index\n",
    "Q4.sampleGenerated = True\n",
    "Q4.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# This is only a sampling of the data, so the dataSample is, by definition, the data\n",
    "Q8.dataSample=Q8.data\n",
    "Q8.filesSample=Q8.dataSample.index\n",
    "# Specify that the sample is a good one\n",
    "Q8.sampleGenerated = True\n",
    "# Specifying that the sample has a TSNE reduction\n",
    "Q8.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q11.dataSample = Q11.data\n",
    "Q11.filesSample =Q11.dataSample.index\n",
    "Q11.sampleGenerated = True\n",
    "Q11.sampleTSNE = True\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_FullSample.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q16.dataSample = Q16.data\n",
    "Q16.filesSample =Q16.dataSample.index\n",
    "Q16.sampleGenerated = True\n",
    "Q16.sampleTSNE = True\n",
    "\n",
    "Q_dict = {'Q8':Q8,'Q4':Q4,'Q11':Q11,'Q16':Q16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    149789.000000\n",
       "mean         -0.027031\n",
       "std           0.162175\n",
       "min          -1.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max           0.000000\n",
       "Name: db_out, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q4.dataSample.db_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the features created with keplerml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User defined\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "\n",
    "#Generating an initial sample of 10,000 light curves\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "#Q4_Sample = Q4.randSampleWTabby(10000)\n",
    "\n",
    "\"\"\"Q4.sample_tsne_fit()\n",
    "print(\"K-means\")\n",
    "Q4.sample_km_out()\n",
    "print(\"DBSCAN\")\n",
    "Q4.sample_db_out()\"\"\"\n",
    "# The Kepler IDs of each lc, without the timestamp, to identify in other quarters\n",
    "kID = [i[:13] for i in Q4.data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8 imported.\n",
      "Q11 imported.\n",
      "Q16 imported.\n"
     ]
    }
   ],
   "source": [
    "# User defined\n",
    "\"\"\"\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# The Kepler IDs of each lc, without the timestamp, to identify in other quarters\n",
    "kID = [i[:13] for i in Q4.data.index]\n",
    "# \"\"\"\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "# Creating a data sample set using the kIDs from the original quarter\n",
    "Q8_common = Q8.data[Q8.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q8_common.index]\n",
    "print(\"Q8 imported.\")\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q11_common = Q11.data[Q11.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q11_common.index]\n",
    "print(\"Q11 imported.\")\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_output.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "Q16_common = Q16.data[Q16.data.index.str.contains('|'.join(kID))]\n",
    "kID = [i[:13] for i in Q16_common.index]\n",
    "print(\"Q16 imported.\")\n",
    "# Ensure all quarters contain the same data\n",
    "Q4_common = Q4.data[Q4.data.index.str.contains('|'.join(kID))]\n",
    "Q8_common = Q8_common[Q8_common.index.str.contains('|'.join(kID))]\n",
    "Q11_common = Q11_common[Q11_common.index.str.contains('|'.join(kID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q4.dataSample = Q4_common\n",
    "Q4.filesSample = Q4.dataSample.index\n",
    "Q4.sampleGenerated = True\n",
    "\n",
    "Q8.dataSample = Q8_common\n",
    "Q8.filesSample = Q8.dataSample.index\n",
    "Q8.sampleGenerated = True\n",
    "\n",
    "Q11.dataSample = Q11_common\n",
    "Q11.filesSample = Q11.dataSample.index\n",
    "Q11.sampleGenerated = True\n",
    "\n",
    "Q16.dataSample = Q16_common\n",
    "Q16.filesSample = Q16.dataSample.index\n",
    "Q16.sampleGenerated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "Sampling data for parameter estimation...\n",
      "Calculating nearest neighbor distances...\n",
      "Finding elbow...\n",
      "\n",
      "        Epsilon is in the neighborhood of 5.58912316903.\n",
      "        \n",
      "Neighbors scaled to 14.\n",
      "Clustering data with DBSCAN...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-195913e38353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mQ4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'db_out'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Q4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-195913e38353>\u001b[0m in \u001b[0;36mcluster_sample\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mscaledData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdf_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdbout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_tabby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdbout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dgiles/Documents/KeplerML/python/clusterOutliers.pyc\u001b[0m in \u001b[0;36mdb_out\u001b[1;34m(self, df, check_tabby, verbose)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdb_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_tabby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mclusterLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_outliers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbscan_w_outliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_tabby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mclusterLabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dgiles/Documents/KeplerML/python/db_outliers.pyc\u001b[0m in \u001b[0;36mdbscan_w_outliers\u001b[1;34m(data, min_n, check_tabby, verbose)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Clustering data with DBSCAN...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdbEps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mclusterLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "def cluster_sample(q):\n",
    "    \n",
    "    Qob = Q_dict[q]\n",
    "    Q = Qob.dataSample\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "    dbout = Qob.db_out(df_scaled,check_tabby=True)\n",
    "    \n",
    "    return dbout\n",
    "\n",
    "start = datetime.now()\n",
    "Q4.dataSample['db_out'] = cluster_sample('Q4')\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data for parameter estimation...\n",
      "Calculating nearest neighbor distances...\n",
      "Finding elbow...\n",
      "\n",
      "        Epsilon is in the neighborhood of 11.0023882682.\n",
      "        \n",
      "Neighbors scaled to 59.\n",
      "(11.002388268239091, 59)\n"
     ]
    }
   ],
   "source": [
    "reload(db_outliers)\n",
    "\n",
    "def eps_workspace(data,n,verbose=True):\n",
    "    X=Q4.dataSample.iloc[:,0:60]\n",
    "    if len(X)>10000:\n",
    "            # Large datasets have presented issues where a single high density cluster \n",
    "            # leads to an epsilon of 0.0 for 4 neighbors.\n",
    "            # We adjust for this by calculating epsilon with 4 neighbors\n",
    "            # for a sample of the data, then we scale min_neighbors accordingly.\n",
    "            if verbose:print(\"Sampling data for parameter estimation...\")\n",
    "            X_sample = data.sample(n=10000)\n",
    "    else:\n",
    "        X_sample = data\n",
    "    #X_sample = data\n",
    "    dbEps,dists = db_outliers.eps_est(X_sample,n=n,verbose=True)\n",
    "    \n",
    "    if len(X)>10000:\n",
    "    # Scale neighbors to account for sampling 10,000 from the data\n",
    "        neighbors = int(n*len(data)/10000)\n",
    "        if verbose:print(\"Neighbors scaled to %s.\"%neighbors)\n",
    "    else:\n",
    "        neighbors = n\n",
    "    #neighbors = n\n",
    "    return dbEps,neighbors,dists\n",
    "\n",
    "def scale_df(Qdf):\n",
    "    data = Qdf.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Qdf.columns[0:60],index = Qdf.index,data=scaledData)\n",
    "    return df_scaled\n",
    "\n",
    "eps,n,dist_arr = eps_workspace(scale_df(Q4.dataSample),4)\n",
    "print(eps,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distArr=dist_arr\n",
    "pts = range(len(distArr))\n",
    "\n",
    "# The following looks for the first instance (past the mid point)\n",
    "# where the mean of the following [number] points\n",
    "# is at least (cutoff-1)*100% greater than the mean of the previous [number] points.\n",
    "\n",
    "number = int(np.ceil(len(pts)/500.))\n",
    "cutoff = 1.05\n",
    "\n",
    "for i in range(int(np.ceil(len(pts)/2)),len(pts)-number):\n",
    "    if np.mean(distArr[i+1:i+number])>=cutoff*np.mean(distArr[i-number:i-1]):\n",
    "        dbEps = distArr[i]\n",
    "        pt=pts[i]\n",
    "        break\n",
    "print(np.median(distArr))\n",
    "plt.scatter(range(len(dist_arr)),dist_arr)\n",
    "plt.scatter(pt,dist_arr[pt],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "        Epsilon is in the neighborhood of 21.763520029.\n",
      "        \n",
      "Clustering data with DBSCAN...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "def cluster_sample(q):\n",
    "    \n",
    "    Qob = Q_dict[q]\n",
    "    Q = Qob.dataSample\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "    \n",
    "    \n",
    "    return Qob.db_out(df_scaled,check_tabby=True)\n",
    "\n",
    "start = datetime.now()\n",
    "Q4.dataSample['db_out'] = cluster_sample('Q4')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q8.dataSample['db_out'] = cluster_sample('Q8')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q11.dataSample['db_out'] = cluster_sample('Q11')\n",
    "print(datetime.now()-start)\n",
    "\n",
    "start = datetime.now()\n",
    "Q16.dataSample['db_out'] = cluster_sample('Q16')\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for q in Q_dict:\n",
    "    pca = Q_dict[q].pca_fit(Q_dict[q].dataSample.iloc[:,0:60])\n",
    "    Q_dict[q].dataSample['pca_x'] = pca.T[0]\n",
    "    Q_dict[q].dataSample['pca_y'] = pca.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering data...\n",
      "\n",
      "    Epsilon is in the neighborhood of 0.824638713357, \n",
      "    with an average of 1999.0 neighbors within epsilon,\n",
      "    999.5 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "Tabby has not Not found to be an outlier in DBSCAN\n",
      "There were 1 clusters and 30590 total outliers\n",
      "0:21:23.518559\n"
     ]
    }
   ],
   "source": [
    "starttime= datetime.now()\n",
    "Q4.db_out(Q4.dataSample[['pca_x','pca_y']])\n",
    "print(datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 6.94193875491, \n",
      "    with an average of 790.0 neighbors within epsilon,\n",
      "    395.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "Tabby has not Not found to be an outlier in DBSCAN\n",
      "There were 1 clusters and 213 total outliers\n",
      "0:00:06.940155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 7.10991455506, \n",
      "    with an average of 1121.0 neighbors within epsilon,\n",
      "    560.5 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "Tabby has not Not found to be an outlier in DBSCAN\n",
      "There were 1 clusters and 236 total outliers\n",
      "0:00:13.435256\n",
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 6.12068023561, \n",
      "    with an average of 880.0 neighbors within epsilon,\n",
      "    440.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "There were 1 clusters and 372 total outliers\n",
      "0:00:07.660973\n",
      "Estimating Parameters...\n",
      "\n",
      "    Epsilon is in the neighborhood of 5.96482020724, \n",
      "    with an average of 810.0 neighbors within epsilon,\n",
      "    405.0 neighbors in half circle (neighbors/2).\n",
      "    \n",
      "Clustering data with DBSCAN...\n",
      "There were 1 clusters and 356 total outliers\n",
      "0:00:06.505793\n"
     ]
    }
   ],
   "source": [
    "def cluster_sample(q):\n",
    "    start = datetime.now()\n",
    "    Q = q.outliers\n",
    "    data = Q.iloc[:,0:60]\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    scaledData = scaler.transform(data)\n",
    "    df_scaled = pd.DataFrame(columns=Q.columns[0:60],index = Q.index,data=scaledData)\n",
    "    db_labels = q.db_out(df_scaled)\n",
    "    print(datetime.now()-start)\n",
    "\n",
    "    return db_labels\n",
    "\n",
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers = Q.dataSample[Q.dataSample.db_out == -1]\n",
    "    Q.outliers['out_out'] = cluster_sample(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1183 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3949\n",
      "[t-SNE] Computed conditional probabilities for sample 3949 / 3949\n",
      "[t-SNE] Mean sigma: 2.278248\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.300924\n",
      "[t-SNE] Error after 100 iterations: 0.300924\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 1681 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5605\n",
      "[t-SNE] Computed conditional probabilities for sample 5605 / 5605\n",
      "[t-SNE] Mean sigma: 2.112685\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 0.210839\n",
      "[t-SNE] Error after 125 iterations: 0.210839\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1318 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4396\n",
      "[t-SNE] Computed conditional probabilities for sample 4396 / 4396\n",
      "[t-SNE] Mean sigma: 2.159918\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.259040\n",
      "[t-SNE] Error after 100 iterations: 0.259040\n",
      "Done.\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1213 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4049\n",
      "[t-SNE] Computed conditional probabilities for sample 4049 / 4049\n",
      "[t-SNE] Mean sigma: 2.169621\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.286553\n",
      "[t-SNE] Error after 100 iterations: 0.286553\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    tsne_fit = Q.tsne_fit(Q.outliers.iloc[:,0:60])\n",
    "    Q.outliers['tsne_x'] = tsne_fit.T[0]\n",
    "    Q.outliers['tsne_y'] = tsne_fit.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting.\n",
      "window closed.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "Q = Q_dict['Q16']\n",
    "Q.plot_sample(Q.outliers,Q.fitsDir,Q.outliers.out_out,reduction_method='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = Q_dict['Q8']\n",
    "inliers = Q.outliers[Q.outliers.out_out==0]\n",
    "outliers = Q.outliers[Q.outliers.out_out==-1]\n",
    "plt.hexbin(inliers.tsne_x,inliers.tsne_y,mincnt=5,bins=\"log\",cmap = \"viridis\",gridsize=35)\n",
    "plt.scatter(outliers.tsne_x,outliers.tsne_y,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to outliers csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers.to_csv(\"/home/dgiles/Documents/KeplerLCs/output/\"+q+\"_outliers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outliers reclustered, re-tsne'd\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q4_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q4fitsfiles\" # path to fits files\n",
    "Q4 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\" # path to fits files\n",
    "Q8 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q11_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q11fitsfiles\" # path to fits files\n",
    "Q11 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q16_outliers.csv\" # Path to csv containing feature data (should be a pandas dataframe saved as a csv)\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q16fitsfiles\" # path to fits files\n",
    "Q16 = clusterOutliers.clusterOutliers(featCSV,fitsDir)\n",
    "\n",
    "\n",
    "Q_dict = {'Q8':Q8,'Q4':Q4,'Q11':Q11,'Q16':Q16}\n",
    "for q in Q_dict:\n",
    "    Q = Q_dict[q]\n",
    "    Q.outliers = Q.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4049"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q4.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             tsne_x  tsne_y\n",
      "cluster                \n",
      "-1           16      16\n",
      " 0         3841    3841\n",
      " 1           25      25\n",
      " 2            4       4\n",
      " 3           16      16\n",
      " 4           14      14\n",
      " 5           13      13\n",
      " 6           13      13\n",
      " 7            4       4\n",
      " 8            3       3        \n",
      "                tsne_x     tsne_y\n",
      "cluster                      \n",
      "-1       22.819558  10.467281\n",
      " 0        4.037788   4.507165\n",
      " 1        0.217006   0.382377\n",
      " 2        0.050079   0.056157\n",
      " 3        0.123062   0.105542\n",
      " 4        0.095739   0.132884\n",
      " 5        0.052605   0.014126\n",
      " 6        0.032419   0.007703\n",
      " 7        0.035876   0.015066\n",
      " 8        0.030285   0.283041\n",
      "        \n",
      "Plotting.\n",
      "window closed.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import pyfits\n",
    "from matplotlib.widgets import Slider\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import km_outliers\n",
    "root = Tk.Tk()\n",
    "root.wm_title(\"Scatter\")\n",
    "from IPython.display import clear_output\n",
    "from accelerate.cuda import cuda\n",
    "\n",
    "\"\"\"--- import light curve data ---\"\"\"\n",
    "\n",
    "Qdict = {'Q4':Q4,'Q8':Q8,'Q11':Q11,'Q16':Q16}\n",
    "Q = Qdict['Q8']\n",
    "Qdf = Q.data\n",
    "pathtofits = Q.fitsDir\n",
    "files = Qdf.index\n",
    "clusterLabels=Qdf.db_out\n",
    "\n",
    "\n",
    "cmap = matplotlib.cm.ScalarMappable(\\\n",
    "    norm=matplotlib.colors.Normalize(vmin=min(clusterLabels),vmax=max(clusterLabels)),\\\n",
    "    cmap='viridis')\n",
    "# data is an array containing each data point\n",
    "data = np.array(Qdf[['tsne_x','tsne_y']])\n",
    "\n",
    "# tsneX has all the x-coordinates\n",
    "tsneX = Qdf.tsne_x\n",
    "# tsneY has all the y-coordinates\n",
    "tsneY = Qdf.tsne_y\n",
    "\n",
    "data_out = Qdf[clusterLabels==-1]\n",
    "outX = data_out.tsne_x\n",
    "outY = data_out.tsne_y\n",
    "files_out = data_out.index\n",
    "\n",
    "data_cluster = Qdf[clusterLabels!=-1]\n",
    "clusterX = data_cluster.tsne_x\n",
    "clusterY = data_cluster.tsne_y\n",
    "files_cluster = data_cluster.index\n",
    "\n",
    "\n",
    "\"\"\"--- Organizing data and Labels ---\"\"\"\n",
    "\n",
    "\n",
    "if Qdf.index.str.contains('8462852').any():\n",
    "    tabbyInd = list(Qdf.index).index(Qdf[Qdf.index.str.contains('8462852')].index[0])            \n",
    "else:\n",
    "    tabbyInd = 0\n",
    "fig = Figure(figsize=(20,10))\n",
    "\n",
    "# a tk.DrawingArea\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "# Toolbar to help navigate the data (pan, zoom, save image, etc.)\n",
    "toolbar = NavigationToolbar2TkAgg(canvas, root)\n",
    "toolbar.update()\n",
    "canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "\n",
    "gs = gridspec.GridSpec(22,6)\n",
    "\n",
    "def eps_est(data,n):\n",
    "    \n",
    "    # distance array containing all distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=int(np.ceil(.1*len(data))), algorithm='ball_tree',n_jobs=-1).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    # Distance to 2*N/100th instead of 4th because: ... reasons\n",
    "    neighbors = n\n",
    "    distArr = distances[:,neighbors]\n",
    "    distArr.sort()\n",
    "    pts = range(len(distArr))\n",
    "\n",
    "    # The following looks for the first instance (past the mid point)\n",
    "    # where the mean of the following [number] points\n",
    "    # is at least (cutoff-1)*100% greater than the mean of the previous [number] points.\n",
    "    \n",
    "    number = 5\n",
    "    cutoff = 1.05\n",
    "    for i in range(int(np.ceil(len(pts)/2)),len(pts)-number):\n",
    "        if np.mean(distArr[i+1:i+number])>=cutoff*np.mean(distArr[i-number:i-1]):\n",
    "            dbEps = distArr[i]\n",
    "            pt=pts[i]\n",
    "            break\n",
    "            \n",
    "    return dbEps\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    # empty subplot for scattered data\n",
    "    ax = fig.add_subplot(gs[0:7,:4])\n",
    "    # empty subplot for lightcurves\n",
    "    ax2 = fig.add_subplot(gs[9:18,:])\n",
    "    # empty subplot for center detail\n",
    "    ax3 = fig.add_subplot(gs[0:7,4:])\n",
    "    \n",
    "    min_points = 4\n",
    "    eps = eps_est(data,min_points)\n",
    "    ax_s_min_n = fig.add_subplot(gs[20,:])\n",
    "    ax_s_eps = fig.add_subplot(gs[21,:])\n",
    "    s_neighbors = Slider(ax_s_min_n,'Neighbors',1,50,valinit=min_points,valfmt='%2i')\n",
    "    s_eps = Slider(ax_s_eps,'Epsilon',.01,10,valinit=eps,valfmt='%8.3f')\n",
    "    \n",
    "# Set those labels\n",
    "ax.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "ax.set_ylabel(\"Reduced Y\",fontsize=18)\n",
    "\n",
    "ax3.set_title(\"Center Density Detail\")\n",
    "ax3.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "ax3.set_ylabel(\"Reduced Y\",fontsize=18)    \n",
    "\n",
    "def read_kepler_curve(file):\n",
    "    \"\"\"\n",
    "    Given the path of a fits file, this will extract the light curve and normalize it.\n",
    "    \"\"\"\n",
    "\n",
    "    lc = pyfits.getdata(file)\n",
    "    t = lc.field('TIME')\n",
    "    f = lc.field('PDCSAP_FLUX')\n",
    "    err = lc.field('PDCSAP_FLUX_ERR')\n",
    "\n",
    "    err = err[np.isfinite(t)]\n",
    "    f = f[np.isfinite(t)]\n",
    "    t = t[np.isfinite(t)]\n",
    "    err = err[np.isfinite(f)]\n",
    "    t = t[np.isfinite(f)]\n",
    "    f = f[np.isfinite(f)]\n",
    "    err = err/np.median(f)\n",
    "    nf = f / np.median(f)\n",
    "\n",
    "    return t, nf, err\n",
    "    \n",
    "@cuda.jit\n",
    "def distance_cuda(dx,dy,dd):\n",
    "    bx = cuda.blockIdx.x # which block in the grid?\n",
    "    bw = cuda.blockDim.x # what is the size of a block?\n",
    "    tx = cuda.threadIdx.x # unique thread ID within a blcok\n",
    "    i = tx + bx * bw + 1\n",
    "    if i>len(dd):\n",
    "        return\n",
    "\n",
    "    # d_ph is a distance placeholder\n",
    "    d_ph = (dx[i]-dx[0])**2+(dy[i]-dy[0])**2\n",
    "\n",
    "    dd[i]=d_ph**.5\n",
    "    return\n",
    "\n",
    "def distances(pts,ex,ey):\n",
    "    # Calculates distances between N points\n",
    "    pts = np.array(pts)\n",
    "    N=len(pts)\n",
    "\n",
    "    # Allocate host memory arrays\n",
    "    # Transpose pts array to n_dims x n_pts, each index of x contains all of a dimensions coordinates\n",
    "    XT = np.transpose(pts)\n",
    "    x = np.array(XT[0])\n",
    "    x = np.insert(x,0,ex)\n",
    "    y = np.array(XT[1])\n",
    "    y = np.insert(y,0,ey)\n",
    "    d = np.zeros(N+1)\n",
    "\n",
    "\n",
    "    # Allocate and copy GPU/device memory\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_d = cuda.to_device(d)\n",
    "\n",
    "    threads_per_block = 128\n",
    "    number_of_blocks =N/128+1 \n",
    "\n",
    "    distance_cuda [ number_of_blocks, threads_per_block ] (d_x,d_y,d_d)\n",
    "\n",
    "    d_d.copy_to_host(d)\n",
    "    return d[1:]   \n",
    "\n",
    "def calcClosestDatapoint(X, event):\n",
    "    \"\"\"Calculate which data point is closest to the mouse position.\n",
    "\n",
    "    Args:\n",
    "        X (np.array) - array of points, of shape (numPoints, 2)\n",
    "        event (MouseEvent) - mouse event (containing mouse position)\n",
    "    Returns:\n",
    "        smallestIndex (int) - the index (into the array of points X) of the element closest to the mouse position\n",
    "    \"\"\"\n",
    "    ex,ey = ax.transData.inverted().transform((event.x,event.y))\n",
    "\n",
    "    #distances = [distance (XT[:,i], event) for i in range(XT.shape[1])]\n",
    "    Ds = distances(X,ex,ey)\n",
    "    return np.argmin(Ds)\n",
    "\n",
    "def drawData(index):\n",
    "    # Plots the lightcurve of the point chosen\n",
    "    ax2.cla()\n",
    "    f = pathtofits+Qdf.index[index]\n",
    "    t,nf,err=read_kepler_curve(f)\n",
    "\n",
    "    x=t\n",
    "    y=nf\n",
    "\n",
    "    axrange=0.55*(max(y)-min(y))\n",
    "    mid=(max(y)+min(y))/2\n",
    "    yaxmin = mid-axrange\n",
    "    yaxmax = mid+axrange\n",
    "    if yaxmin < .95:\n",
    "        if yaxmax > 1.05:\n",
    "            ax2.set_ylim(yaxmin,yaxmax)\n",
    "        else:\n",
    "            ax2.set_ylim(yaxmin,1.05)\n",
    "    elif yaxmax > 1.05:\n",
    "        ax2.set_ylim(.95,yaxmax)\n",
    "    else:\n",
    "        ax2.set_ylim(.95,1.05)\n",
    "\n",
    "    if clusterLabels[index]==-1:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = cmap.to_rgba(clusterLabels[index])\n",
    "        \n",
    "    ax2.plot(x, y, 'o',markeredgecolor='none', c=color, alpha=0.2)\n",
    "    ax2.plot(x, y, '-',markeredgecolor='none', c=color, alpha=0.7)\n",
    "    #ax2.set_title(files[index][:13],fontsize = 20)\n",
    "    ax2.set_xlabel('Time (Days)',fontsize=22)\n",
    "    ax2.set_ylabel(r'$\\frac{\\Delta F}{F}$',fontsize=30)\n",
    "\n",
    "    fig.suptitle(files[index][:13],fontsize=30)\n",
    "\n",
    "    canvas.draw()\n",
    "\n",
    "def annotatePt(XT, index,ignore=False):\n",
    "    \"\"\"Create popover label in 3d chart\n",
    "\n",
    "    Args:\n",
    "        X (np.array) - array of points, of shape (numPoints, 3)\n",
    "        index (int) - index (into points array X) of item which should be printed\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    x2, y2 = XT[index][0], XT[index][1]\n",
    "    # Either update the position, or create the annotation\n",
    "    if not ignore:\n",
    "        if hasattr(annotatePt, 'label'):\n",
    "            annotatePt.label.remove()\n",
    "            annotatePt.emph.remove()\n",
    "        if hasattr(annotatePt, 'emphCD'):\n",
    "            annotatePt.emphCD.remove()\n",
    "    \n",
    "    x_ax = ax.get_xlim()[1]-ax.get_xlim()[0]\n",
    "    y_ax = ax.get_ylim()[1]-ax.get_ylim()[0]\n",
    "    # Get data point from array of points X, at position index\n",
    "    annotatePt.label = ax.annotate( \"\",\n",
    "        xy = (x2, y2), xytext = (x2+.1*x_ax, y2+.2*y_ax),\n",
    "        arrowprops = dict(headlength=20,headwidth=20,width=6,shrink=.1,color='red'))\n",
    "    annotatePt.emph = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "    if files[index] in files_cluster:\n",
    "        annotatePt.emphCD = ax3.scatter(x2,y2,marker='o',s=150,c='red')\n",
    "    else:\n",
    "        annotatePt.emphCD = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "    canvas.draw()\n",
    "\n",
    "\n",
    "def onMouseClick(event, X):\n",
    "    \"\"\"Event that is triggered when mouse is clicked. Shows lightcurve for data point closest to mouse.\"\"\"\n",
    "    #XT = np.array(X.T) # array organized by feature, each in it's own array\n",
    "    closestIndex = calcClosestDatapoint(X, event)\n",
    "    drawData(closestIndex)\n",
    "\n",
    "def onMouseRelease(event, X):\n",
    "    #XT = np.array(X.T)\n",
    "    closestIndex = calcClosestDatapoint(X, event)\n",
    "    annotatePt(X,closestIndex)\n",
    "    #for centerIndex in centerIndices:\n",
    "    #    annotateCenter(XT,centerIndex)\n",
    "\n",
    "def connect(X):\n",
    "    if hasattr(connect,'cidpress'):\n",
    "        fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "    if hasattr(connect,'cidrelease'):\n",
    "        fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "    \n",
    "    connect.cidpress = fig.canvas.mpl_connect('button_press_event', lambda event: onMouseClick(event,X))\n",
    "    connect.cidrelease = fig.canvas.mpl_connect('button_release_event', lambda event: onMouseRelease(event, X))\n",
    "    \n",
    "def enter_axes(event):\n",
    "    if event.inaxes == ax:\n",
    "        connect(data)\n",
    "\n",
    "def leave_axes(event):\n",
    "    if event.inaxes == ax:\n",
    "        fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "        fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "\n",
    "def replot():\n",
    "    ax.cla()\n",
    "    ax3.cla()\n",
    "    global cmap\n",
    "    cmap = matplotlib.cm.ScalarMappable(\\\n",
    "            norm=matplotlib.colors.Normalize(vmin=0,vmax=max(clusterLabels)),\\\n",
    "            cmap='viridis')\n",
    "    data_out = Qdf[clusterLabels==-1]\n",
    "    outX = data_out.tsne_x\n",
    "    outY = data_out.tsne_y\n",
    "    # Set those labels\n",
    "    ax.set_xlabel(\"Reduced X\",fontsize=18)\n",
    "    ax.set_ylabel(\"Reduced Y\",fontsize=18)\n",
    "    # Scatter the data\n",
    "    ax.scatter(Qdf[clusterLabels!=-1].tsne_x,Qdf[clusterLabels!=-1].tsne_y,\n",
    "               c=clusterLabels[clusterLabels!=-1],cmap=\"viridis\",alpha=.5)\n",
    "    ax.scatter(outX,outY,c='red',s=30,alpha=.8)\n",
    "    ax.annotate(\"Outliers: %6i\"%len(clusterLabels[clusterLabels==-1]),\n",
    "               xy=(0,0),textcoords=\"axes fraction\",xytext=(0.05,.9))\n",
    "    ax.annotate(\"Clusters: %6i\"%(max(clusterLabels)+1),\n",
    "               xy=(0,0),textcoords=\"axes fraction\",xytext=(0.05,.85))\n",
    "\n",
    "    ax3.scatter(Qdf[clusterLabels!=-1].tsne_x,Qdf[clusterLabels!=-1].tsne_y,\n",
    "            c=clusterLabels[clusterLabels!=-1],cmap=\"viridis\",alpha=.2)\n",
    "    annotatePt(data,tabbyInd,ignore=True)\n",
    "    \n",
    "def recluster_km(X_scaled,k):\n",
    "\n",
    "    global clusterLabels\n",
    "    clusterLabels = km_outliers.kmeans_w_outliers(X_scaled,min_samples)\n",
    "    replot()\n",
    "    \n",
    "    clear_output()\n",
    "    df = pd.DataFrame({'tsne_x':X_scaled.tsne_x,\n",
    "                       'tsne_y':X_scaled.tsne_y,\n",
    "                       'cluster':clusterLabels})\n",
    "    \n",
    "    for i in range(max(clusterLabels)+1):\n",
    "        center = [sum(Qdf[clusterLabels==i].tsne_x)/len(Qdf[clusterLabels==i]),\n",
    "                  sum(Qdf[clusterLabels==i].tsne_y)/len(Qdf[clusterLabels==i])]\n",
    "        distFromCenter = [sum((pt-center)**2)**.5 for pt in np.array(Qdf[clusterLabels==i][['tsne_x','tsne_y']])]\n",
    "        sigma = np.std(distFromCenter)\n",
    "        cutoff = 4*sigma\n",
    "        circle = plt.Circle((center[0],center[1]),cutoff,edgecolor=None,alpha=.2)\n",
    "        ax.add_artist(circle)\n",
    "    \n",
    "    return\n",
    "\n",
    "def recluster_db(X_scaled,eps,min_samples):\n",
    "    ax.cla()\n",
    "    ax3.cla()\n",
    "    est = DBSCAN(eps=eps,min_samples=min_samples)\n",
    "    est.fit(X_scaled)\n",
    "\n",
    "    global clusterLabels\n",
    "    clusterLabels = est.labels_\n",
    "    \n",
    "    clear_output()\n",
    "    df = pd.DataFrame({'tsne_x':X_scaled.tsne_x,\n",
    "                       'tsne_y':X_scaled.tsne_y,\n",
    "                       'cluster':clusterLabels})\n",
    "    gb = df.groupby('cluster')\n",
    "    print(\"\"\"\n",
    "    %s        \n",
    "    %s\n",
    "        \"\"\"%(gb.count(),gb.std()))\n",
    "    replot()\n",
    "    return\n",
    "\n",
    "def update_eps(val):\n",
    "    eps = s_eps.val\n",
    "    min_samples = int(s_neighbors.val)\n",
    "\n",
    "    #X = Qdf.iloc[:,0:60]\n",
    "    #scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #scaledData = scaler.transform(X)\n",
    "    #X_scaled = pd.DataFrame(columns=X.columns[0:60],index = X.index,data=scaledData)\n",
    "    \n",
    "    X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "    recluster_db(X_scaled,eps,min_samples)\n",
    "    return\n",
    "\n",
    "    \n",
    "def update_neighbors(val):\n",
    "    min_samples = int(s_neighbors.val)\n",
    "    \n",
    "    #X = Qdf.iloc[:,0:60]\n",
    "    #scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #scaledData = scaler.transform(X)\n",
    "    #X_scaled = pd.DataFrame(columns=X.columns[0:60],index = X.index,data=scaledData)\n",
    "    \n",
    "    X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "    # Automatically determines a suggestion for epsilon based on the number of neighbors chosen\n",
    "    eps = eps_est(X_scaled,min_samples)\n",
    "    # the following changes eps and initiates update_eps\n",
    "    s_eps.set_val(eps)\n",
    "    \n",
    "    return\n",
    "        \n",
    "fig.canvas.mpl_connect('axes_enter_event',enter_axes)\n",
    "fig.canvas.mpl_connect('axes_leave_event',leave_axes)\n",
    "s_eps.on_changed(update_eps)\n",
    "s_neighbors.on_changed(update_neighbors)\n",
    "s_neighbors.set_val(4)\n",
    "annotatePt(data,tabbyInd)\n",
    "drawData(tabbyInd)\n",
    "\n",
    "canvas.draw()\n",
    "canvas.show()\n",
    "print(\"Plotting.\")         \n",
    "\n",
    "def _delete_window():\n",
    "    print(\"window closed.\")\n",
    "    root.destroy()\n",
    "    sys.exit()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\",_delete_window)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further down the rabbit hole..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 22 outliers in 1 clusters\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 1207 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4027\n",
      "[t-SNE] Computed conditional probabilities for sample 4027 / 4027\n",
      "[t-SNE] Mean sigma: 2.265101\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 0.267778\n",
      "[t-SNE] Error after 125 iterations: 0.267778\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dgiles/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "Q = Qdict['Q4']\n",
    "Qdf = Q.outliers\n",
    "X_scaled = Qdf[['tsne_x','tsne_y']]\n",
    "Qdf.cluster = km_outliers.kmeans_w_outliers(X_scaled,1)\n",
    "Q_rh = Qdf[Qdf.cluster==0]\n",
    "tsne_fit = Q.tsne_fit(Q_rh.iloc[:,0:60])\n",
    "Q_rh['tsne_x'] = tsne_fit.T[0]\n",
    "Q_rh['tsne_y'] = tsne_fit.T[1]\n",
    "plt.scatter(Q_rh.tsne_x,Q_rh.tsne_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
